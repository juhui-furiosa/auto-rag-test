vectordb:
  - name: furiosa_store
    db_type: chroma
    client_type: persistent
    embedding_model: openai_embed_3_large
    collection_name: furiosa_default
    path: vector_store/default

node_lines:
  - node_line_name: retrieve_node_line
    nodes:
      - node_type: lexical_retrieval
        top_k: 5
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        modules:
          - module_type: bm25
            bm25_tokenizer: [porter_stemmer, space]
      - node_type: semantic_retrieval
        top_k: 5
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        modules:
          - module_type: vectordb
            vectordb: furiosa_store
      - node_type: hybrid_retrieval
        top_k: 5
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision, retrieval_ndcg, retrieval_mrr]
        modules:
          - module_type: hybrid_rrf
            weight_range: (4, 80)
  - node_line_name: post_retrieve_node_line
    nodes:
      - node_type: prompt_maker
        strategy:
          metrics:
            - metric_name: meteor
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai_embed_3_large
        modules:
          - module_type: fstring
            prompt: "Read the passages and answer the given question.\\nQuestion: {query}\\nPassage: {retrieved_contents}\\nAnswer:"
      - node_type: generator
        strategy:
          metrics:
            - metric_name: meteor
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai_embed_3_large
        modules:
          - module_type: llama_index_llm
            llm: openai
            model: [gpt-4o]
            batch: 8
