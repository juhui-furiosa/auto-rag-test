{
  "title": "Model Parallelism",
  "url": "https://developer.furiosa.ai/latest/en/furiosa_llm/model-parallelism.html",
  "version": "2025.3.1",
  "content": "# Model Parallelism[#](#model-parallelism \"Link to this heading\")\n\nModel parallelism is a technique used to split a model into multiple parts\nand run them in parallel across multiple devices.\nIt’s particularly useful for handling large models that don’t fit in a single\ndevice or for improving the throughput or latency of model inference.\n\nThere are three types of model parallelism: tensor parallelism (TP),\npipeline parallelism (PP), and data parallelism (DP).\nEach type has its own benefits and drawbacks.\nAdditionally, it is possible to combine multiple types of parallelism to achieve\nbetter performance.\n\nIn this section, we explain the three types of model parallelism, how to\nconfigure them in Furiosa-LLM, and provide best practices.\n\n## Tensor Parallelism (TP)[#](#tensor-parallelism-tp \"Link to this heading\")\n\nTensor Parallelism (TP) splits each layer into multiple chunks along a specific dimension.\nEach device only holds 1/N of the whole layer, without affecting the correctness\nof the computation graph.\n\nSince only a portion of the model and intermediate results reside on each\ndevice, TP reduces the memory requirements for weights, KV cache, and activation\nmemory in each device.\nHence, TP is particularly useful when the model size exceeds the memory capacity\nof a single device.\nIncreased memory availability also allows larger batch sizes and longer\nsequences.\n\nAdditionally, TP can reduce the latency of model inference by leveraging\nthe aggregate computation power and memory bandwidth of multiple devices.\n\nHowever, TP requires extra communication to synchronize data across multiple\ndevices.\nThis operation, known as collective communication (e.g., all-reduce and\nall-gather), ensures data consistency across all devices.\nIf the degree of TP is too high, the communication overhead can hurt\nperformance, leading to slower model inference.\n\n## Pipeline Parallelism (PP)[#](#pipeline-parallelism-pp \"Link to this heading\")\n\nPipeline Parallelism (PP) splits the model vertically (usually at layer level)\nacross multiple devices, so that only a subset of the model layers reside on a\nsingle device.\nEach device processes a different part, or “stage”, of the model, passing the\nintermediate results to the next device.\n\nLike tensor parallelism, PP also reduces the memory footprint of a single\ndevice.\nThis is because only a portion of the model and its intermediate results are\nstored on each device.\n\nUnlike TP, PP increases the throughput of model inference at the cost of\nincreased latency\nSince each device processes a different part of the model sequentially, the\nend-to-end latency is determined by the accumulated latency of each stage.\nHowever, PP increases the throughput by processing multiple requests in\nparallel.\n\n## Data Parallelism (DP)[#](#data-parallelism-dp \"Link to this heading\")\n\nData parallelism (DP) is based on the SPMD (single program multiple data)\nprogramming model.\nIt is the most common form of parallelism due to its simplicity.\nFor inference, DP replicates the same model instance multiple times,\nwith each replica independently processing a fraction of the input requests.\nAs a result, DP is highly effective for scaling out the throughput of model\ninference.\n\nEach device requires a separate copy of the model weights and KV cache.\nTherefore, memory usage scales with the number of replicas.\nHowever, when using DP within a single FuriosaAI device, the weights of replicas\ncan be shared due to the shared memory architecture of the device.\n\n### Configuring Model Parallelism in Furiosa-LLM[#](#configuring-model-parallelism-in-furiosa-llm \"Link to this heading\")\n\nThe ArtifactBuilder API and the `furiosa-llm build` command allows to configure the degree of tensor parallelism\nby specifying the `--tensor_parallel_size` or `-tp` option.\n\n`pipeline_parallel_size` and `data_parallel_size` can be configured when loading a model artifact.\nFor example, when you run `furiosa-llm serve`, `furiosa-mlperf` command,\nyou can specify `--pipeline-parallel-size` (or `-pp`) and `--data-parallel-size` (or `-dp`) options.\nSimilarly, `LLMEngine`, `AsyncLLMEngine` allows to specify the degree of pipeline parallelism and data parallelism.\n\n### Best Practices for Furiosa-LLM[#](#best-practices-for-furiosa-llm \"Link to this heading\")\n\nEach parallelism type has its own characteristics and constraints.\n\nThe `pipeline_parallel_size` parameter can be at most the number of devices.\nThe `tensor_parallel_size` parameter can only be 4 or 8 in the 2025.3.1 release;\nfuture releases will lift this limitation.\n\nThe product of `tensor_parallel_size`, `pipeline_parallel_size`, and `data_parallel_size`\nshould be equal to the total number of PEs in your machine.\n\nFor example, let’s assume that you have 4 RNGD cards, each of which has 8 PEs.\nThen, the total number of PEs is 32.\nIn that case, you can use the following configurations:\n\n* `tensor_parallel_size=8`, `pipeline_parallel_size=4`, `data_parallel_size=1`\n* `tensor_parallel_size=4`, `pipeline_parallel_size=2`, `data_parallel_size=2`",
  "external_links": []
}