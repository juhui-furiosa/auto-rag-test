{
  "title": "Model Preparation",
  "url": "https://developer.furiosa.ai/latest/en/furiosa_llm/model-preparation.html",
  "version": "2025.3.1",
  "content": "# Model Preparation[#](#model-preparation \"Link to this heading\")\n\nTo run an LLM model on the Furiosa NPU, Furiosa-LLM must convert the model into a model artifact.\nDuring the conversion process, Furiosa-LLM applies a variety of\noptimizations to enable high-performance inference. This document describes the overall workflow for\npreparing a model artifact from an LLM model and deploying it.\n\nTip\n\nThis section is intended for users who wish to prepare their own model artifacts\nfor further optimization or customization. If you are looking for a quick start,\nplease refer to the [Quick Start with Furiosa-LLM](../get_started/furiosa_llm.html#gettingstartedfuriosallm) section.\nAdditionally, Furiosa-LLM provides a set of pre-compiled model artifacts for popular LLMs in the\n[Hugging Face Hub ðŸ¤— - FuriosaAI organization](https://huggingface.co/furiosa-ai).\nYou can use these to quickly run LLM models on the Furiosa NPU.\n\n## Prerequisites[#](#prerequisites \"Link to this heading\")\n\nEnsure that you meet the following prerequisites before starting the model preparation workflow:\n\n* A system with the prerequisites installed (see [Installing Prerequisites](../get_started/prerequisites.html#installingprerequisites))\n* An installation of [Furiosa-LLM](../get_started/furiosa_llm.html#installingfuriosallm)\n* A [Hugging Face access token](../get_started/furiosa_llm.html#authorizinghuggingfacehub)\n* Sufficient storage space for model weights (varies depending on the model size)\n\n## Authorizing Hugging Face Hub (Optional)[#](#authorizing-hugging-face-hub-optional \"Link to this heading\")\n\nSome models, such as `meta-llama/Llama-3.1-8B`, require a license to run.\nFor these models, you need to create a Hugging Face account, accept the modelâ€™s\nlicense, and generate an access token.\nYou can create your access token at <https://huggingface.co/settings/tokens>.\nOnce you get the access token, you can authenticate on the Hugging Face Hub as follows:\n\n```\npip install --upgrade \"huggingface_hub[cli]\"\nhuggingface-cli login --token $HF_TOKEN\n```\n\nCopy to clipboard\n\n## Download a model from Hugging Face Hub (Optional)[#](#download-a-model-from-hugging-face-hub-optional \"Link to this heading\")\n\nWhen using a model from the Hugging Face Hub, Furiosa-LLM automatically downloads the model weights\nduring the artifact building process.\nHowever, depending on your network environment, downloading the model weights may take a long time.\nIf you want to download a model from the Hugging Face Hub in advance, you can do so using the\n`huggingface-cli` command.\n\nThe following command downloads the model weights and configuration files\nfor the Llama 3.1 8B model to the Hugging Face Hub cache directory.\n\n```\nhuggingface-cli download \"meta-llama/Llama-3.1-8B-Instruct\"\n```\n\nCopy to clipboard\n\n## Optimize and Convert Models to a Model Artifact[#](#optimize-and-convert-models-to-a-model-artifact \"Link to this heading\")\n\nFuriosa-LLM provides a command-line tool, `furiosa-llm build`, to optimize and convert models\ninto model artifacts.\n\nThe following shows an example for building a model artifact for `meta-llama/Llama-3.1-8B-Instruct`\nand saving it to the `./Output-Llama-3.1-8B-Instruct` directory. The `-tp` option specifies the tensor parallelism degree,\nand `--max-seq-len-to-capture` defines the maximum sequence length that the model can handle.\n\n```\nfuriosa-llm build meta-llama/Llama-3.1-8B-Instruct \\\n    ./Output-Llama-3.1-8B-Instruct \\\n    --tensor-parallel-size 8 \\\n    --max-seq-len-to-capture 4096\n```\n\nCopy to clipboard\n\nOnce a model artifact is built, you can deploy it to any machine equipped with FuriosaAI RNGD and\nrun the model using the [LLM class](reference/llm.html#llmclass) or the appropriate interface like [OpenAI-Compatible Server](furiosa-llm-serve.html#openaiserver).\n\nTip\n\nTo achieve better performance or to run LLM models on multiple NPUs,\nyou can take advantage of model parallelism in Furiosa-LLM. To learn more about model\nparallelism, please refer to the [Model Parallelism](model-parallelism.html#modelparallelism) section.\n\nYou can also build a model artifact using the [ArtifactBuilder](reference/artifact_builder.html#artifactbuilderclass) API.\nHere is an example using the [ArtifactBuilder](reference/artifact_builder.html#artifactbuilderclass) API.\n\n```\nfrom furiosa_llm.artifact.builder import ArtifactBuilder\n\ncompiled_model = \"./Output-Llama-3.1-8B-Instruct\"\n\nbuilder = ArtifactBuilder(\n        \"meta-llama/Llama-3.1-8B-Instruct\",\n        tensor_parallel_size=8,\n        max_seq_len_to_capture=4096, # Maximum sequence length covered by LLM engine\n)\nbuilder.build(compiled_model)\n```\n\nCopy to clipboard\n\nBoth `furiosa-llm build` and [ArtifactBuilder](reference/artifact_builder.html#artifactbuilderclass) API offer a variety of options to customize the model artifact.\nYou can specify the tensor parallelism degree, pipeline parallelism degree, data parallelism degree,\nprefill and decode bucket sizes, and other options. Please refer to\n[Building Model Artifacts By Examples](build-artifacts-by-examples.html#buildingmodelartifactsbyexamples) section for more examples and details.\n\n## Model Quantization (Optional)[#](#model-quantization-optional \"Link to this heading\")\n\nQuantization is a widely used technique to reduce the computational and memory requirements for inference\nby mapping the high-precision space of activations, weights, and KV cache to lower-precision formats\nsuch as INT8, FP8, or INT4 â€” while aiming to preserve model accuracy.\n\nIt is typically applied when higher throughput or lower latency is needed. However, since quantization may affect\nmodel accuracy, it is important to perform thorough experimentation and accuracy evaluations.\n\nFuriosa-LLM currently supports Post-Training Quantization (PTQ) for model quantization.\nTo apply PTQ, you need to calibrate the model using a calibration dataset and then export the quantized model as a checkpoint.\nThe following sections explain the PTQ workflow with Furiosa-LLM.\n\n### Load a Model to Quantize[#](#load-a-model-to-quantize \"Link to this heading\")\n\nThe first step is to prepare a model to quantize.\nThe `QuantizerForCausalLM` class provides a simple API to load a\nmodel from either the Hugging Face Hub or a local path.\n`QuantizerForCausalLM` is a subclass of [AutoModelForCausalLM](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM),\nso it automatically determines the model class from the Hugging Face model ID in the same way as\n`AutoModelForCausalLM`.\n\n```\nfrom furiosa_llm.optimum import QuantizerForCausalLM\n\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\nquantizer = QuantizerForCausalLM.from_pretrained(model_id)\n```\n\nCopy to clipboard\n\n### Calibrate and Quantize the Model[#](#calibrate-and-quantize-the-model \"Link to this heading\")\n\nOnce a model is loaded, you can calibrate and quantize it by calling the\n`QuantizerForCausalLM.quantize()` method.\n\nThe `quantize()` method takes as arguments the model to be quantized, a data loader, and a\nquantization configuration. The `create_data_loader` function helps generate a data loader\nthat supplies the quantization process with an appropriate sample dataset.\nWhen creating a data loader, you can configure parameters such as the tokenizer, dataset name or path, dataset split, number of samples, and maximum sample length. These parameters can significantly impact the accuracy of the quantized model, so some experimentation is typically necessary to determine the optimal settings.\n\nTip\n\n`create_data_loader` is based on the `datasets` library,\nwhich provides easy access to datasets for tasks in audio, computer vision, and\nnatural language processing (NLP).\nLearn more in the\n[datasets documentation](https://huggingface.co/docs/datasets/en/index)\nand explore the available datasets at <https://huggingface.co/datasets>.\n\nThe following example demonstrates how to create a data loader for the calibration dataset.\nThe quantized model will be saved to the `save_dir` directory.\n\n```\nfrom furiosa_llm.optimum.dataset_utils import create_data_loader\nfrom furiosa_llm.optimum import QuantizerForCausalLM, QuantizationConfig\n\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n\n# Create a dataloader for calibration\ndataloader = create_data_loader(\n    tokenizer=model_id,\n    dataset_name_or_path=\"mit-han-lab/pile-val-backup\",\n    dataset_split=\"validation\",\n    num_samples=5, # Increase this number for better calibration\n    max_sample_length=1024,\n)\n\nquantized_model = \"./quantized_model\"\n# Load a pre-trained model from Hugging Face model hub\nquantizer = QuantizerForCausalLM.from_pretrained(model_id)\n# Calibrate, quantize the model, and save the quantized model\nquantizer.quantize(quantized_model, dataloader, QuantizationConfig.w_f8_a_f8_kv_f8())\n```\n\nCopy to clipboard\n\nThe `QuantizationConfig` class allows you to specify various quantization options\nand offers a set of pre-defined quantization configurations.\nFor example, `QuantizationConfig.w_f8_a_f8_kv_f8()`, quantizes the weights, activations, and KV cache to 8-bit floating-point (FP8).\n\nOnce you have the quantized model, you can create a model artifact using either the\n`ArtifactBuilder` API or the `furiosa-llm build` command. Below is an example of using the\n`furiosa-llm build` command to generate a model artifact from the quantized model.\n\n```\nfrom furiosa_llm.artifact.builder import ArtifactBuilder\n\nquantized_model = \"./quantized_model\"\ncompiled_model = \"./Output-Llama-3.1-8B-Instruct\"\n\nbuilder = ArtifactBuilder(\n        quantized_model,\n        tensor_parallel_size=8,  # Tensor parallel sing 8PEs\n        max_seq_len_to_capture=1024, # Maximum sequence length covered by LLM engine\n)\nbuilder.build(compiled_model)\n```\n\nCopy to clipboard\n\n## Deploying Model Artifacts[#](#deploying-model-artifacts \"Link to this heading\")\n\nOnce you have a model artifact, you can transfer and reuse it on any machine with a\nFuriosa NPU and Furiosa-LLM installed.\nTo transfer a model artifact:\n\n1. Compress the model artifact directory using your preferred compression tool.\n2. Copy the compressed file to the target host.\n3. Uncompress it on the target machine.\n4. Run the model using either the [LLM class](reference/llm.html#llmclass) or the [OpenAI-Compatible Server](furiosa-llm-serve.html#openaiserver).\n\nFor quick examples of loading and running model artifacts, refer to the\n[Quick Start with Furiosa-LLM](../get_started/furiosa_llm.html#gettingstartedfuriosallm) section.",
  "external_links": [
    {
      "text": "Hugging Face Hub ðŸ¤— - FuriosaAI organization",
      "url": "https://huggingface.co/furiosa-ai"
    },
    {
      "text": "https://huggingface.co/settings/tokens",
      "url": "https://huggingface.co/settings/tokens"
    },
    {
      "text": "AutoModelForCausalLM",
      "url": "https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM"
    },
    {
      "text": "datasets documentation",
      "url": "https://huggingface.co/docs/datasets/en/index"
    },
    {
      "text": "https://huggingface.co/datasets",
      "url": "https://huggingface.co/datasets"
    }
  ]
}