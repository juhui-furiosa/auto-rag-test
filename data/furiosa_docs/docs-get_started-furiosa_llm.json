{
  "title": "Quick Start with Furiosa-LLM",
  "url": "https://developer.furiosa.ai/latest/en/get_started/furiosa_llm.html",
  "version": "2025.3.1",
  "content": "# Quick Start with Furiosa-LLM[#](#quick-start-with-furiosa-llm \"Link to this heading\")\n\nFuriosa-LLM is a serving framework for LLM models that uses FuriosaAI’s NPU.\nIt provides a Python API compatible with vLLM and a server compatible with\nOpenAI’s API.\nThis document explains how to install and use Furiosa-LLM.\n\nWarning\n\nThis document is based on Furiosa SDK 2025.3.1.\nThe features and APIs described herein are subject to change in the future.\n\n## Installing Furiosa-LLM[#](#installing-furiosa-llm \"Link to this heading\")\n\nThe minimum requirements for Furiosa-LLM are as follows:\n\n* A system with the prerequisites installed (see [Installing Prerequisites](prerequisites.html#installingprerequisites))\n* Python 3.9, 3.10, 3.11, or 3.12\n* PyTorch 2.5.1\n* Sufficient storage space for model weights (varies depending on the model size)\n\nTo install the `furiosa-compiler` package and Furiosa-LLM,\nrun the following commands:\n\n```\nsudo apt install -y furiosa-compiler\n\npip install --upgrade pip setuptools wheel\npip install --upgrade furiosa-llm\n```\n\nCopy to clipboard\n\n### Authorizing Hugging Face Hub (Optional)[#](#authorizing-hugging-face-hub-optional \"Link to this heading\")\n\nSome models, such as meta-llama/Llama-3.1-8B, require a license to run.\nFor these, you need to create a Hugging Face account, accept the model’s\nlicense, and generate a token.\nYou can create your token at <https://huggingface.co/settings/tokens>.\nOnce you get a token, you can authenticate on the Hugging Face Hub as follows:\n\n```\npip install --upgrade \"huggingface_hub[cli]\"\nhuggingface-cli login --token $HF_TOKEN\n```\n\nCopy to clipboard\n\n## Offline Batch Inference with Furiosa-LLM[#](#offline-batch-inference-with-furiosa-llm \"Link to this heading\")\n\nWe now explain how to perform offline LLM inference using the Python API of Furiosa-LLM.\nFirst, import the `LLM` and `SamplingParams` classes from the furiosa\\_llm module.\nThe `LLM` class is used to load LLM models and provides the core API for LLM inference.\n`SamplingParams` is used to specify various parameters for text generation.\n\n```\nfrom furiosa_llm import LLM, SamplingParams\n\n# Load the Llama 3.1 8B Instruct model\nllm = LLM.load_artifact(\"furiosa-ai/Llama-3.1-8B-Instruct-FP8\", devices=\"npu:0\")\n\n# You can specify various parameters for text generation\nsampling_params = SamplingParams(min_tokens=10, top_p=0.3, top_k=100)\n\n# Prompt for the model\nmessage = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\nprompt = llm.tokenizer.apply_chat_template(message, tokenize=False)\n\n# Generate text\nresponse = llm.generate([prompt], sampling_params)\n\n# Print the output of the model\nprint(response[0].outputs[0].text)\n```\n\nCopy to clipboard\n\n## Streaming Inference with Furiosa-LLM[#](#streaming-inference-with-furiosa-llm \"Link to this heading\")\n\nIn addition to batch inference, Furiosa-LLM also supports streaming inference.\nThe key difference of streaming inference is that tokens are returned as soon\nthey are generated.\nThis allows you to start printing or processing partial tokens before the whole\ninference process finishes.\nTo perform streaming inference, use the `stream_generate` method instead of\n`generate`.\nThis method is asynchronous and returns a stream of tokens as they are generated.\n\n```\nimport asyncio\nfrom furiosa_llm import LLM, SamplingParams\n\nasync def main():\n    # Load the Llama 3.1 8B Instruct model\n    llm = LLM.load_artifact(\"furiosa-ai/Llama-3.1-8B-Instruct-FP8\", devices=\"npu:0\")\n\n    # You can specify various parameters for text generation\n    sampling_params = SamplingParams(min_tokens=10, top_p=0.3, top_k=100)\n\n    # Prompt for the model\n    message = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n    prompt = llm.tokenizer.apply_chat_template(message, tokenize=False)\n\n    # Generate text and print each token at a time\n    async for output_txt in llm.stream_generate(prompt, sampling_params):\n        print(output_txt, end=\"\", flush=True)\n\n# Run the async main function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nCopy to clipboard\n\n## Chat Inference with Furiosa-LLM[#](#chat-inference-with-furiosa-llm \"Link to this heading\")\n\nFuriosa-LLM provides a high-level `chat` method for models with chat capabilities.\nThis method simplifies interactions by handling prompt templating internally - it applies the appropriate chat template\nand invokes the `generate` method with the formatted prompt.\n\nFor detailed examples of using the chat API, refer to:\n\n* [Chat](../furiosa_llm/examples/llm_chat.html#furiosallmexampleschat)\n* [Chat with tools](../furiosa_llm/examples/llm_chat_with_tools.html#furiosallmexampleschatwithtools)\n\n## Launching the OpenAI-Compatible Server[#](#launching-the-openai-compatible-server \"Link to this heading\")\n\nFuriosa-LLM can be deployed as a server that provides an API compatible with\nOpenAI’s.\nSince many LLM frameworks and applications are built on top of OpenAI’s API,\nyou can easily integrate Furiosa-LLM into your existing applications.\n\nBy default, the server listens on the HTTP endpoint <http://localhost:8000>.\nYou can change the binding address and port by specifying the `--host` and `--port` options.\nThe server can host only one model at a time for now and provides a chat template feature.\nYou can find more details in the [OpenAI-Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) section.\n\nBelow is an example of how to launch the server with the Llama 3.1 8B Instruct model.\n\n```\n# Launch the server to listen 8000 port by default\nfuriosa-llm serve furiosa-ai/Llama-3.1-8B-Instruct-FP8 --devices \"npu:0\"\n```\n\nCopy to clipboard\n\nThe server loads the model and starts listening on the specified port.\nWhen the server is ready, you will see the following message:\n\n```\nINFO:     Started server process [27507]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n```\n\nCopy to clipboard\n\nThen, you can test the server using the following curl command:\n\n```\ncurl http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n    \"model\": \"EMPTY\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n    }' \\\n    | python -m json.tool\n```\n\nCopy to clipboard\n\nExample output:\n\n```\n{\n  \"id\": \"chat-21f0b74b2c6040d3b615c04cb5bf2e2e\",\n  \"object\": \"chat.completion\",\n  \"created\": 1736480800,\n  \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n  \"choices\": [\n      {\n          \"index\": 0,\n          \"message\": {\n              \"role\": \"assistant\",\n              \"content\": \"The capital of France is Paris.\",\n              \"tool_calls\": []\n          },\n          \"logprobs\": null,\n          \"finish_reason\": \"stop\",\n          \"stop_reason\": null\n      }\n  ],\n  \"usage\": {\n      \"prompt_tokens\": 42,\n      \"total_tokens\": 49,\n      \"completion_tokens\": 7\n  },\n  \"prompt_logprobs\": null\n}\n```\n\nCopy to clipboard",
  "external_links": [
    {
      "text": "https://huggingface.co/settings/tokens",
      "url": "https://huggingface.co/settings/tokens"
    },
    {
      "text": "http://localhost:8000",
      "url": "http://localhost:8000"
    }
  ]
}