{
  "title": "OpenAI-Compatible Server",
  "url": "https://developer.furiosa.ai/latest/en/furiosa_llm/furiosa-llm-serve.html",
  "version": "2025.3.1",
  "content": "# OpenAI-Compatible Server[#](#openai-compatible-server \"Link to this heading\")\n\nIn addition to the Python API, Furiosa LLVM offers an OpenAI-compatible server\nthat hosts a single model and provides two OpenAI-compatible APIs:\n[Completions API](https://platform.openai.com/docs/api-reference/completions) and\n[Chat API](https://platform.openai.com/docs/api-reference/chat).\n\nTo launch the server, use the `furiosa-llm serve` command with the model\nartifact path, as follows:\n\n```\nfuriosa-llm serve [ARTIFACT_PATH]\n```\n\nCopy to clipboard\n\nThe following sections describe how to launch and configure the server\nand interact with the server using OpenAI API clients.\n\nWarning\n\nThis document is based on Furiosa SDK 2025.3.1.\nThe features and APIs described herein are subject to change in the future.\n\n## Prerequisites[#](#prerequisites \"Link to this heading\")\n\nTo use the OpenAI-Compatible server, you need the following:\n\n* A system with the prerequisites installed (see [Installing Prerequisites](../get_started/prerequisites.html#installingprerequisites))\n* An installation of [Furiosa-LLM](../get_started/furiosa_llm.html#installingfuriosallm)\n* A [Hugging Face access token](../get_started/furiosa_llm.html#authorizinghuggingfacehub)\n* A model artifact\n* Chat template for chat application (Optional)\n\n## Using the OpenAI API[#](#using-the-openai-api \"Link to this heading\")\n\nOnce the server is running, you can interact with it using an HTTP client,\nas shown in the following example:\n\n```\ncurl http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n    \"model\": \"EMPTY\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n    }' \\\n    | python -m json.tool\n```\n\nCopy to clipboard\n\nYou can also use the OpenAI client to interact with the server.\nTo use the OpenAI client, you need to install the `openai` package first:\n\n```\npip install openai\n```\n\nCopy to clipboard\n\nThe OpenAI client provides two APIs: `client.chat.completions` and\n`client.completions`.\nTo stream responses, you can use the `client.chat.completions`\nAPI with `stream=True`, as follows:\n\n```\nimport asyncio\nfrom openai import AsyncOpenAI\n\n# Replace the following with your base URL\nbase_url = \"http://localhost:8000/v1\"\napi_key = \"EMPTY\"\n\nclient = AsyncOpenAI(api_key=api_key, base_url=base_url)\n\nasync def run():\n    stream_chat_completion = await client.chat.completions.create(\n        model=\"EMPTY\",\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        stream=True,\n    )\n\n    async for chunk in stream_chat_completion:\n        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run())\n```\n\nCopy to clipboard\n\nBy default, the Furiosa-LLM server binds to `localhost:8000`.\nYou can change the host and port using the `--host` and `--port` options.\n\n## Chat Templates[#](#chat-templates \"Link to this heading\")\n\nTo use a language model in a chat application, we need to prepare a structured\nstring to give as input.\nThis is essential because the model must understand the conversation’s context,\nincluding the speaker’s role (e.g., “user” and “assistant”) and the\nmessage content.\nJust as different models require distinct tokenization methods, they also have\nvarying input formats for chat.\nThis is why a chat template is necessary.\n\nFuriosa-LLM supports chat templates based on the Jinja2 template engine, similar\nto Hugging Face Transformers.\nIf the model’s tokenizer includes a built-in chat template,\n`furiosa-llm serve` will automatically use it.\nHowever, if the tokenizer lacks a built-in template, or if you want to override\nthe default, you can specify one using the `--chat-template` parameter.\n\nFor reference, you can find a well-structured example of a chat template in the\n[Llama 3.1 Model Card](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTo launch the server with a custom chat template, use the following command:\n\n```\nfuriosa-llm serve [ARTIFACT_PATH] --chat-template [CHAT_TEMPLATE_PATH]\n```\n\nCopy to clipboard\n\n## Tool Calling Support[#](#tool-calling-support \"Link to this heading\")\n\nFuriosa-LLM supports tool calling (also known as function calling) for models\ntrained with this capability.\n\nThe system converts model outputs into the OpenAI response format through a\ndesignated parser implementation.\nAt this time, only the `llama3_json` parser is available.\nAdditional parsers will be introduced in future releases.\n\nThe following example command starts the server with tool calling enabled for Llama 3.1 models:\n\n```\nfuriosa-llm serve furiosa-ai/Llama-3.1-8B-Instruct-FP8 --devices \"npu:0\" \\\n  --enable-auto-tool-choice --tool-call-parser llama3_json\n```\n\nCopy to clipboard\n\nTo use the tool calling feature, specify the `tools` and `tool_choice`\nparameters. Here’s an example:\n\n```\nfrom openai import OpenAI\nimport json\n\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"test\")\n\ndef get_weather(location: str, unit: str):\n    return f\"Getting the weather for {location} in {unit}...\"\ntool_functions = {\"get_weather\": get_weather}\n\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City and state, e.g., 'San Francisco, CA'\"},\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n            },\n            \"required\": [\"location\", \"unit\"]\n        }\n    }\n}]\n\nresponse = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}],\n    tools=tools,\n    tool_choice=\"auto\" # None is also equivalent to \"auto\"\n)\n\ntool_call = response.choices[0].message.tool_calls[0].function\nprint(f\"Function called: {tool_call.name}\")\nprint(f\"Arguments: {tool_call.arguments}\")\nprint(f\"Result: {get_weather(**json.loads(tool_call.arguments))}\")\n```\n\nCopy to clipboard\n\nThe expected output is as follows.\n\n```\nFunction called: get_weather\nArguments: {\"location\": \"San Francisco, CA\", \"unit\": \"fahrenheit\"}\nResult: Getting the weather for San Francisco, CA in fahrenheit...\n```\n\nCopy to clipboard\n\n## Reasoning Support[#](#reasoning-support \"Link to this heading\")\n\nFuriosa-LLM provides support for models with reasoning capabilities, such as Deepseek R1 series. These models follow a structured approach by first conducting reasoning steps and then providing a final answer.\n\nThe reasoning process follows this sequence:\n\n* The model-specific start-of-reasoning token is appended to the input prompt through the chat template.\n* The model generates its reasoning.\n* Once reasoning is complete, the model outputs an end-of-reasoning token followed by the final answer.\n\nSince start-of-reasoning and end-of-reasoning tokens are model-specific, we support different reasoning parsers for different models.\nCurrently, `deepseek_r1` parser is available. This parser expects `<think>` and `</think>` as the start-of-reasoning and end-of-reasoning tokens respectively.\nAny models that follow the same token scheme (such as Qwen QWQ) can use this parser.\n\nTo launch a server with reasoning capabilities for Deepseek R1 series, use the following example command:\n\n```\nfuriosa-llm serve furiosa-ai/DeepSeek-R1-Distill-Llama-8B --devices \"npu:0\" \\\n  --reasoning-parser deepseek_r1\n```\n\nCopy to clipboard\n\nYou can access the reasoning content through these response fields:\n\n* `response.choices[].message.reasoning_content`\n* `response.choices[].delta.reasoning_content`\n\nHere’s an example that demonstrates how to access the reasoning content:\n\n```\nfrom openai import OpenAI\n\n# Replace the following with your base URL\nbase_url = \"http://localhost:8000/v1\"\napi_key = \"EMPTY\"\n\nclient = OpenAI(api_key=api_key, base_url=base_url)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}]\nresponse = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=messages\n)\n\nif hasattr(response.choices[0].message, \"reasoning_content\"):\n    print(\"Reasoning:\", response.choices[0].message.reasoning_content)\nprint(\"Answer:\", response.choices[0].message.content)\n```\n\nCopy to clipboard\n\nNote\n\nThe `reasoning_content` field is a Furiosa-LLM-specific extension and is not part of the standard OpenAI API.\nThis field will appear only in responses that contain reasoning content, and\nattempting to access this field in responses without reasoning content will raise an `AttributeError`.\n\n## Supported OpenAI API Parameters[#](#supported-openai-api-parameters \"Link to this heading\")\n\nThe following table outlines the supported parameters for Chat and Completions APIs.\nAny parameters not listed in the table are unsupported and will be ignored by the server.\n\nWarning\n\nPlease note that using `use_beam_search` together with `stream` is not\nallowed because beam search requires the whole sequence to produce the\noutput tokens.\n\n### Chat API (`POST /v1/chat/completions`)[#](#chat-api-post-v1-chat-completions \"Link to this heading\")\n\nParameters without descriptions inherit their behavior and functionality from the corresponding parameters in [OpenAI Chat API](https://platform.openai.com/docs/api-reference/chat).\n\n| Name | Type | Default | Description |\n| --- | --- | --- | --- |\n| model | string |  | Required by the client, but the value is ignored on the server. |\n| messages | array |  |  |\n| stream | boolean | false |  |\n| stream\\_options | object | null |  |\n| n | integer | 1 | Currently limited to 1. |\n| temperature | float | 1.0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| top\\_p | float | 1.0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| best\\_of | integer | 1 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| use\\_beam\\_search | boolean | false | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| top\\_k | integer | -1 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| min\\_p | float | 0.0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| length\\_penalty | float | 1.0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| early\\_stopping | boolean | false | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| min\\_tokens | integer | 0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| max\\_tokens | integer | null | Legacy parameter superseded by `max_completion_tokens` |\n| max\\_completion\\_tokens | integer | null | If null, the server will use the maximum possible length considering the prompt. The sum of this value and the prompt length must not exceed the model’s maximum context length. |\n| tools | array | null |  |\n| tool\\_choice | string or object | null |  |\n| functions | array | null | Legacy parameter superseded by `tools`. |\n| function\\_call | string or object | null | Legacy parameter superseded by `tool_choice`. |\n| logprobs (experimental) | boolean | false |  |\n| top\\_logprobs (experimental) | integer | null |  |\n\n### Completions API (`POST /v1/completions`)[#](#completions-api-post-v1-completions \"Link to this heading\")\n\nParameters without descriptions inherit their behavior and functionality from the corresponding parameters in [OpenAI Completions API](https://platform.openai.com/docs/api-reference/completions).\n\n| Name | Type | Default | Description |\n| --- | --- | --- | --- |\n| model | string | required | Required by the client, but the value is ignored on the server. |\n| prompt | string or array | required |  |\n| stream | boolean | false |  |\n| stream\\_options | object | null |  |\n| n | integer | 1 | Currently limited to 1. |\n| best\\_of | integer | 1 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| temperature | float | 1.0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| top\\_p | float | 1.0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| use\\_beam\\_search | boolean | false | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| top\\_k | integer | -1 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| min\\_p | float | 0.0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| length\\_penalty | float | 1.0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| early\\_stopping | boolean | false | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| min\\_tokens | integer | 0 | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n| max\\_tokens | integer | 16 |  |\n| logprobs (experimental) | integer | null | See [Sampling Params](reference/sampling_params.html#samplingparams). |\n\n## Additional API Endpoints[#](#additional-api-endpoints \"Link to this heading\")\n\nIn addition to the Chat and Completions APIs, the Furiosa-LLM server supports the following endpoints.\n\n### Models Endpoint[#](#models-endpoint \"Link to this heading\")\n\nThe Models API enables you to retrieve information about available models through endpoints that are compatible with OpenAI’s [Models API](https://platform.openai.com/docs/api-reference/models). The following endpoints are supported:\n\n* `GET /v1/models`\n* `GET /v1/models/{model_id}`\n\nYou can access these endpoints using the OpenAI client’s `models.list()` and `models.retrieve()` methods.\n\nThe response includes the standard [model object](https://platform.openai.com/docs/api-reference/models/object) as defined by OpenAI, along with the following Furiosa-LLM-specific extensions:\n\n* `artifact_id`: Unique identifier for the model artifact.\n* `max_prompt_len`: Maximum allowed length of input prompts.\n* `max_context_len`: Maximum allowed length of the total context window.\n* `runtime_config`: Model runtime configuration parameters, including bucket specifications.\n\n### Version Endpoint[#](#version-endpoint \"Link to this heading\")\n\n`GET /version`\n\nExposes version information for the Furiosa SDK components.\n\n### Metrics Endpoint[#](#metrics-endpoint \"Link to this heading\")\n\n`GET /metrics`\n\nExposes Prometheus-compatible metrics for monitoring server performance and health.\n\nThis endpoint is available when the server is launched with the `--enable-metrics` flag.\nSee [Monitoring the OpenAI-Compatible Server](#monitoringopenaicompatibleserver) for detailed information about available metrics and their usage.\n\n## Monitoring the OpenAI-Compatible Server[#](#monitoring-the-openai-compatible-server \"Link to this heading\")\n\nFuriosa-LLM exposes a Prometheus-compatible metrics endpoint at /metrics,\nwhich provides various metrics compatible with vLLM. These metrics can be\nused to monitor LLM serving workloads and the system health.\nThe metrics endpoint can be enabled with `--enable-metrics` option.\n\nThe following table shows Furiosa-LLM-specific collectors and metrics:\n\n| Metric | Type | Metric Labels | Description |\n| --- | --- | --- | --- |\n| `furiosa_llm_num_requests_running` | Gauge | `model_name` | Number of requests running on RNGD. |\n| `furiosa_llm_num_requests_waiting` | Gauge | `model_name` | Number of requests waiting to be processed. |\n| `furiosa_llm_request_received_total` | Counter | `model_name` | Number of received requests in total. |\n| `furiosa_llm_request_success_total` | Counter | `model_name` | Number of successfully processed requests in total. |\n| `furiosa_llm_request_failure_total` | Counter | `model_name` | Number of request process failures in total. |\n| `furiosa_llm_prompt_tokens_total` | Counter | `model_name` | Total number of prefill tokens processed. |\n| `furiosa_llm_generation_tokens_total` | Counter | `model_name` | Total number of generation tokens processed. |\n| `furiosa_llm_time_to_first_token_seconds` | Histogram | `model_name` | Time to first token (TTFT) in seconds. |\n| `furiosa_llm_time_per_output_token_seconds` | Histogram | `model_name` | Time per output token (TPOT) in seconds. |\n| `furiosa_llm_e2e_request_latency_seconds` | Histogram | `model_name` | End-to-end request latency in seconds. |\n| `furiosa_llm_request_prompt_tokens` | Histogram | `model_name` | Number of prefilled tokens processed per request. |\n| `furiosa_llm_request_generation_tokens` | Histogram | `model_name` | Number of generation tokens processed per request. |\n| `furiosa_llm_request_params_max_tokens` | Histogram | `model_name` | max\\_token request parameter received per request. |\n| `furiosa_llm_kv_cache_used_bytes` | Gauge | `model_name`, `device_index` | Size of used KV-cache in bytes. |\n| `furiosa_llm_kv_cache_total_bytes` | Gauge | `model_name`, `device_index` | Total KV-cache size in bytes. |\n| `furiosa_llm_kv_cache_usage_perc` | Gauge | `model_name`, `device_index` | KV-cache usage. 1 means 100 percent usage. |\n| `furiosa_llm_pipeline_info` | Gauge | `model_name`, `pipeline_id`, `device_id`, `device_index` | Information about each pipeline. |\n\n## Launching the OpenAI-Compatible Server Container[#](#launching-the-openai-compatible-server-container \"Link to this heading\")\n\nFuriosaAI offers a containerized server that can be used for faster deployment.\nHere is an example that launches the Furiosa-LLM server in a Docker container\n(replace `$HF_TOKEN` with your Hugging Face Hub token):\n\n```\ndocker pull furiosaai/furiosa-llm:latest\n\ndocker run -it --rm \\\n  --device /dev/rngd:/dev/rngd \\\n  --security-opt seccomp=unconfined \\\n  --env HF_TOKEN=$HF_TOKEN \\\n  -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n  -p 8000:8000 \\\n  furiosaai/furiosa-llm:latest \\\n  serve furiosa-ai/Llama-3.1-8B-Instruct-FP8 --devices \"npu:0\"\n```\n\nCopy to clipboard\n\nYou can also specify additional options for the server and replace\n`-v $HOME/.cache/huggingface:/root/.cache/huggingface` with the path to your\nHugging Face cache directory.",
  "external_links": [
    {
      "text": "Completions API",
      "url": "https://platform.openai.com/docs/api-reference/completions"
    },
    {
      "text": "Chat API",
      "url": "https://platform.openai.com/docs/api-reference/chat"
    },
    {
      "text": "Llama 3.1 Model Card",
      "url": "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/"
    },
    {
      "text": "OpenAI Chat API",
      "url": "https://platform.openai.com/docs/api-reference/chat"
    },
    {
      "text": "OpenAI Completions API",
      "url": "https://platform.openai.com/docs/api-reference/completions"
    },
    {
      "text": "Models API",
      "url": "https://platform.openai.com/docs/api-reference/models"
    },
    {
      "text": "model object",
      "url": "https://platform.openai.com/docs/api-reference/models/object"
    }
  ]
}