{
  "title": "Deploying Furiosa-LLM on Kubernetes",
  "url": "https://developer.furiosa.ai/latest/en/furiosa_llm/k8s_deployment.html",
  "version": "2025.3.1",
  "content": "# Deploying Furiosa-LLM on Kubernetes[#](#deploying-furiosa-llm-on-kubernetes \"Link to this heading\")\n\nThis guide describes how to deploy Furiosa-LLM—an OpenAI-compatible server optimized for Furiosa AI RNGDs—on Kubernetes.\n\n## Prerequisites[#](#prerequisites \"Link to this heading\")\n\n* A Kubernetes cluster equipped with Furiosa RNGD devices.\n* Hugging Face account and access token.\n* A Kubernetes storage class which supports dynamic volume provisioning.\n\nFor detailed instructions on setting up an RNGD cluster, please refer to [Installing Prerequisites](../get_started/prerequisites.html#installingprerequisites) and [Kubernetes Plugins](../cloud_native_toolkit/kubernetes.html#kubernetes).\n\n## Step 1: Prepare Kubernetes Secret[#](#step-1-prepare-kubernetes-secret \"Link to this heading\")\n\nCreate a Kubernetes secret for your Hugging Face token:\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hf-token-secret\ntype: Opaque\ndata:\n  token: <your_base64_encoded_hf_token>\n```\n\nCopy to clipboard\n\nEncode your token using the following command:\n\n```\necho -n '<your_HF_TOKEN>' | base64\n```\n\nCopy to clipboard\n\nThen apply the secret:\n\n```\nkubectl apply -f secret.yaml\n```\n\nCopy to clipboard\n\n## Step 2: Create Persistent Volume Claim (PVC)[#](#step-2-create-persistent-volume-claim-pvc \"Link to this heading\")\n\nNote\n\nUsing ephemeral storage inside a Pod for large files, such as LLM model caches, may result in eviction due to disk pressure.\nThus, using a Persistent Volume Claim (PVC) is highly recommended to ensure data durability and stability.\n\nCreate a PVC manifest to store models.\nThe storage class specified below (“default”) is an example, and the actual available storage class in your Kubernetes environment may vary.\nPlease verify and specify the appropriate storage class for your cluster:\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: llama-storage\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  # The storage class \"default\" is an example, use an appropriate one for your cluster.\n  storageClassName: default\n```\n\nCopy to clipboard\n\nApply the PVC:\n\n```\nkubectl apply -f pvc.yaml\n```\n\nCopy to clipboard\n\n## Step 3: Deploy Furiosa-LLM Server[#](#step-3-deploy-furiosa-llm-server \"Link to this heading\")\n\nCreate a Deployment manifest for serving the `furiosa-ai/Llama-3.1-8B-Instruct-FP8` model with Furiosa-LLM.\nFor detailed information about the `furiosa-ai/Llama-3.1-8B-Instruct-FP8` model, please see the [Hugging Face model page](https://huggingface.co/furiosa-ai/Llama-3.1-8B-Instruct-FP8).\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llama-3-8b\n  labels:\n    app: llama-3-8b\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: llama-3-8b\n  template:\n    metadata:\n      labels:\n        app: llama-3-8b\n    spec:\n      containers:\n        - name: llama-3-8b\n          image: furiosaai/furiosa-llm:latest\n          args:\n            - \"serve\"\n            - \"furiosa-ai/Llama-3.1-8B-Instruct-FP8\"\n          ports:\n            - containerPort: 8000\n          resources:\n            # Recommended resources for one RNGD card: 10 CPU cores and 100GB memory\n            limits:\n              cpu: 10\n              memory: 100Gi\n              furiosa.ai/rngd: \"1\"\n            requests:\n              cpu: 10\n              memory: 100Gi\n              furiosa.ai/rngd: \"1\"\n          env:\n            - name: HF_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hf-token-secret\n                  key: token\n          volumeMounts:\n            - name: model-storage\n              mountPath: /root/.cache/huggingface\n          securityContext:\n            capabilities:\n              drop:\n                - ALL\n            seccompProfile:\n              type: Unconfined\n          # Increase initialDelaySeconds of livenessProbe and readinessProbe if larger models take longer to load\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8000\n            initialDelaySeconds: 180\n            periodSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8000\n            initialDelaySeconds: 180\n            periodSeconds: 5\n      volumes:\n        # If dynamic PVC provisioning isn’t possible in your cluster, consider using a hostPath volume.\n        - name: model-storage\n          persistentVolumeClaim:\n            claimName: llama-storage\n```\n\nCopy to clipboard\n\nApply the deployment:\n\n```\nkubectl apply -f deployment.yaml\n```\n\nCopy to clipboard\n\n## Step 4: Expose the Deployment as a Service[#](#step-4-expose-the-deployment-as-a-service \"Link to this heading\")\n\nExpose the Furiosa-LLM server using a Kubernetes Service:\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: llama-3-8b\nspec:\n  selector:\n    app: llama-3-8b\n  ports:\n    - port: 8000\n      targetPort: 8000\n  type: ClusterIP\n```\n\nCopy to clipboard\n\nApply the service:\n\n```\nkubectl apply -f service.yaml\n```\n\nCopy to clipboard\n\n## Step 5: Test the Deployment[#](#step-5-test-the-deployment \"Link to this heading\")\n\nConfirm the server is running by inspecting the logs:\n\n```\nkubectl logs deployment/llama-3-8b\n```\n\nCopy to clipboard\n\nYou should see output similar to:\n\n```\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n```\n\nCopy to clipboard\n\nTest the inference endpoint:\n\n```\nkubectl port-forward svc/llama-3-8b 8000:8000\n```\n\nCopy to clipboard\n\nThen, issue a test request:\n\n```\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"furiosa-ai/Llama-3.1-8B-Instruct-FP8\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 7,\n        \"temperature\": 0\n      }'\n```\n\nCopy to clipboard\n\nYou should receive a valid response from the Furiosa-LLM server, similar to:\n\n```\n{\n  \"id\": \"cmpl-69102e5d78c94e29b74660eaadbe39db\",\n  \"object\": \"text_completion\",\n  \"created\": 1748675715,\n  \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" city that is known for its vibrant\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"prompt_logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 12,\n    \"completion_tokens\": 7,\n    \"completion_tokens_details\": null\n  }\n}\n```\n\nCopy to clipboard\n\n## Conclusion[#](#conclusion \"Link to this heading\")\n\nDeploying Furiosa-LLM with Kubernetes leverages the efficiency and scalability of Furiosa RNGD accelerators.\nThis guide provides a straightforward approach to setting up your Kubernetes cluster to serve inference workloads efficiently.",
  "external_links": [
    {
      "text": "Hugging Face model page",
      "url": "https://huggingface.co/furiosa-ai/Llama-3.1-8B-Instruct-FP8"
    }
  ]
}