{
  "title": "Furiosa-LLM",
  "url": "https://developer.furiosa.ai/latest/en/furiosa_llm/intro.html",
  "version": "2025.3.1",
  "content": "# Furiosa-LLM[#](#furiosa-llm \"Link to this heading\")\n\nFuriosa-LLM is a high-performance inference engine for LLM and multi-modal\nLLM models.\nFuriosa-LLM offers state-of-the-art serving efficiency and optimizations.\nKey features of Furiosa-LLM include:\n\n* vLLM-compatible API (LLM, LLMEngine, AsyncLLMEngine API)\n* Efficient KV cache management with PagedAttention\n* Continuous batching of incoming requests\n* Quantization: FP8 (Planned: INT4, INT8, GPTQ, AWQ)\n* Support for data, tensor, and pipeline parallelism across multiple NPUs\n* OpenAI-compatible API server\n* Various decoding algorithms: greedy search, beam search, top-k/top-p, and\n  speculative decoding (planned for 2025.3)\n* Support for context lengths of up to 32k\n* Tool calling and reasoning parser support\n* Structured output generation with guided decoding (guided\\_choice, guided\\_regex, guided\\_json, guided\\_grammar)\n* Chunked Prefill\n* Integration with Hugging Face models and hub support\n* Hugging Face PEFT support (planned)\n\n## Documentation[#](#documentation \"Link to this heading\")\n\n* [Quick Start with Furiosa-LLM](../get_started/furiosa_llm.html#gettingstartedfuriosallm): A quick start guide to Furiosa-LLM\n* [OpenAI-Compatible Server](furiosa-llm-serve.html#openaiserver): Details about the OpenAI-compatible server and its features\n* [Structured Output](structured-output.html#structuredoutput): Guide to structured output generation with guided decoding\n* [Model Preparation](model-preparation.html#modelpreparation): How to prepare LLM models to be served by Furiosa-LLM\n* [Building Model Artifacts By Examples](build-artifacts-by-examples.html#buildingmodelartifactsbyexamples): A guide to building model artifacts through examples\n* [Model Parallelism](model-parallelism.html#modelparallelism): A guide to model parallelism in Furiosa-LLM\n* [API Reference](reference.html#furiosallmreference): Python API reference for Furiosa-LLM\n* [Examples](examples.html#furiosallmexamples): Examples of using Furiosa-LLM\n* [Deploying Furiosa-LLM on Kubernetes](k8s_deployment.html#k8sfuriosallmdeployment): A guide to deploying Furiosa-LLM on Kubernetes",
  "external_links": []
}