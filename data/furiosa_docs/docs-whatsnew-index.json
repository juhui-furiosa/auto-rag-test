{
  "title": "What‚Äôs New",
  "url": "https://developer.furiosa.ai/latest/en/whatsnew/index.html",
  "version": "2025.3.1",
  "content": "# What‚Äôs New[#](#what-s-new \"Link to this heading\")\n\nTable of Contents\n\n* [Furiosa SDK 2025.3.1 Beta3 (2025-08-25)](#furiosa-sdk-2025-3-1-beta3-2025-08-25)\n* [Furiosa SDK 2025.3.0 Beta3 (2025-08-04)](#furiosa-sdk-2025-3-0-beta3-2025-08-04)\n* [Furiosa SDK 2025.2.0 Beta2 (2025-04-25)](#furiosa-sdk-2025-2-0-beta2-2025-04-25)\n* [Furiosa SDK 2025.1.0 Beta1 (2025-02-24)](#furiosa-sdk-2025-1-0-beta1-2025-02-24)\n* [Furiosa SDK 2024.2.1 Beta0 (2025-01-10)](#furiosa-sdk-2024-2-1-beta0-2025-01-10)\n* [Furiosa SDK 2024.2.0 Beta0 (2024-12-23)](#furiosa-sdk-2024-2-0-beta0-2024-12-23)\n* [Furiosa SDK 2024.1.0 Alpha (2024-10-11)](#furiosa-sdk-2024-1-0-alpha-2024-10-11)\n\n## Furiosa SDK 2025.3.1 Beta3 (2025-08-25)[#](#furiosa-sdk-2025-3-1-beta3-2025-08-25 \"Link to this heading\")\n\nThe 2025.3.1 release is a minor update but introduces several important new features.\nBecause these features are added at the Furiosa-LLM frontend, they do not introduce\nany breaking changes. This release also includes a number of bug fixes.\n\nPlease refer to the [Upgrading FuriosaAI‚Äôs Software](../get_started/upgrade_guide.html#upgradeguide) section for instructions on\nobtaining this update.\n\n### üöÄ Highlights[#](#highlights \"Link to this heading\")\n\n#### Features & Improvements[#](#features-improvements \"Link to this heading\")\n\n* Print out the log of average throughput, KV cache usages, and running/waiting requests regularly.\n* Expose the more production metrics (running/waiting requests, total KV cache and usage) through `/metrics` endpoint.\n* Fix the compilation error of small LLMs like Qwen2.5 7B, 14B.\n* Fix the bug that occurs when initializing the runtime multiple times in the same Python interpreter.\n* Support `tool_choice: \"required\"` and `tool_choice` with a named function.\n* Support the structured output with `guided_choice`, `guided_regex`, `guided_json`, and `guided_grammar` (see [Structured Output](../furiosa_llm/structured-output.html#structuredoutput)).\n* Add llguidance as the default guided decoding backend.\n* Bundle a set of NPU programs as a single zip file.\n* Allow to quantize and build the fine-tuned models.\n\n#### Expanded Model Support[#](#expanded-model-support \"Link to this heading\")\n\n* Added support for additional Qwen 2.5 family models:\n\n  + [Qwen2.5-32B-Instruct](https://huggingface.co/furiosa-ai/Qwen2.5-32B-Instruct)\n  + [Qwen2.5-Coder-14B-Instruct](https://huggingface.co/furiosa-ai/Qwen2.5-Coder-14B-Instruct)\n\n### üö® Breaking Changes[#](#breaking-changes \"Link to this heading\")\n\n* No breaking change at all\n\n## Furiosa SDK 2025.3.0 Beta3 (2025-08-04)[#](#furiosa-sdk-2025-3-0-beta3-2025-08-04 \"Link to this heading\")\n\nThe 2025.3.0 release focuses on inter-chip tensor parallelism and performance optimizations.\nTheese efforts have resulted in dramatic performance improvements, including an up to 3x throughput increase\nfor Llama 3.1 70B and an up to 55% reduction in first-token latency for Llama 3.1 8B.\nThis release also introduces support for the Qwen2 and Qwen2.5 models, as well as W8A16 quantization.\n\nTo update your SDK, please refer to [Upgrading FuriosaAI‚Äôs Software](../get_started/upgrade_guide.html#upgradeguide).\n\n### üöÄ Highlights[#](#release2025-3-0-highlights \"Link to this heading\")\n\n#### Major Features & Improvements[#](#major-features-improvements \"Link to this heading\")\n\n* Inter-chip Tensor Parallelism\n\n  + Tensor parallelism across multiple NPU cards is now officially supported, enabling efficient\n    scaling of large models and leading to significantly improved throughput.\n  + To maximize performance, this feature is backed by key optimizations including:\n    optimized PCIe paths for peer-to-peer (P2P) communication, advanced communication scheduling,\n    and compiler tactics that overlap inter-chip DMA with computation.\n* Compiler and Runtime Optimizations\n\n  + Enhanced the Furiosa compiler‚Äôs global optimization\n    capabilities to maximize SRAM reuse between transformer blocks,\n    reducing memory access latency and boosting overall throughput.\n  + Reducing interference between the host and the NPU to further optimize the runtime,\n    improving synchronization across devices and minimizing overhead between consecutive\n    decoding steps\n\n#### Performance Highlights[#](#performance-highlights \"Link to this heading\")\n\nThe optimizations in this release yield the following performance improvements compared\nto the previous 2025.2.0 release:\n\n* Llama 3.1 8B: Up to a 4.5% average throughput improvement and up to a 55% average reduction in\n  Time-to-First-Token (TTFT)\n* Llama 3.1 70B: Up to 3x average throughput improvement and up to a 35% average reduction in\n  Time-to-First-Token (TTFT)\n* The experiment configuration is as follows:\n\n  + input lengths: 1k ~ 12k\n  + output lengths: 128 ~ 10k\n  + batch size: 1 ~ 128\n\n#### Expanded Model Support[#](#id2 \"Link to this heading\")\n\n* Support for Qwen2 and Qwen2.5 models\n  (e.g., [Qwen2.5-Coder-32B-Instruct](https://huggingface.co/furiosa-ai/Qwen2.5-Coder-32B-Instruct)).\n* Added support for W8A16 quantization\n  (e.g., [Llama-3.3-70B-Instruct-INT8](https://huggingface.co/furiosa-ai/Llama-3.3-70B-Instruct-INT8)).\n* New pre-compiled artifacts are available on the [Hugging Face Hub](https://huggingface.co/furiosa-ai):\n\n  + [EXAONE-3.5-32B-Instruct](https://huggingface.co/furiosa-ai/EXAONE-3.5-32B-Instruct)\n  + [DeepSeek-R1-Distill-Llama-70B](https://huggingface.co/furiosa-ai/DeepSeek-R1-Distill-Llama-70B)\n  + [Llama-3.3-70B-Instruct](https://huggingface.co/furiosa-ai/Llama-3.3-70B-Instruct)\n  + [Llama-3.3-70B-Instruct-INT8](https://huggingface.co/furiosa-ai/Llama-3.3-70B-Instruct-INT8)\n  + [Qwen2.5-Coder-32B-Instruct](https://huggingface.co/furiosa-ai/Qwen2.5-Coder-32B-Instruct)\n* Longer Context Lengths: all pre-compiled artifacts on [Hugging Face Hub](https://huggingface.co/furiosa-ai) now support\n  context lengths of up to 32k tokens.\n\n### üö® Breaking Changes[#](#id6 \"Link to this heading\")\n\n* SDK 2025.2.0 cannot load artifacts built with 2025.3.x.\n  Please use an artifact built with 2025.3.x, or rebuild the model again with the new SDK.\n* furiosa-mlperf is deprecated and is removed from this release.\n  Please use other benchmark tools, such as [vLLM benchmark](https://github.com/vllm-project/vllm/tree/main/benchmarks)\n  or [LLMPerf](https://github.com/ray-project/llmperf).\n\n## Furiosa SDK 2025.2.0 Beta2 (2025-04-25)[#](#furiosa-sdk-2025-2-0-beta2-2025-04-25 \"Link to this heading\")\n\nThe RNGD SDK 2025.2.0 is the release brings a wide range of new features and significant improvements,\nincluding support for reasoning models, the metrics endpoint, the chat API, the Hugging Face Hub, the abort() API,\nand the chunked prefill feature. 2025.2.0 also enables direct building of bfloat16, float16, and float32 models from\nthe Hugging Face Hub without a quantization step. Additionally, pre-compiled model artifacts are now available\non the Hugging Face Hub, so you can use them immediately without having to build them yourself.\n\nPlease refer to the [Upgrading FuriosaAI‚Äôs Software](../get_started/upgrade_guide.html#upgradeguide) section for instructions on\nobtaining this update.\n\n### üöÄ Highlights[#](#release2025-2-0-highlights \"Link to this heading\")\n\n* Add support for [stream\\_options.include\\_usage](https://community.openai.com/t/usage-stats-now-available-when-using-streaming-with-the-chat-completions-api-or-completions-api/738156) in [OpenAI-Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver).\n* Introduce `LLM.chat()` API to support chat-based models (see [Chat](../furiosa_llm/examples/llm_chat.html#furiosallmexampleschat), [Chat with tools](../furiosa_llm/examples/llm_chat_with_tools.html#furiosallmexampleschatwithtools)).\n* Mitigate out-of-memory issue by setting the default value of `spare_block_ratio=0` in [OpenAI-Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver).\n* Fix a bug caused by duplicate buckets in `furiosa-llm`.\n* Add support for `/v1/models` and `/v1/models/{model_id}` endpoints in `furiosa-llm` (see [Models Endpoint](../furiosa_llm/furiosa-llm-serve.html#modelsendpoint)).\n* Add support for `/version` endpoint in [OpenAI-Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) (see [Version Endpoint](../furiosa_llm/furiosa-llm-serve.html#versionendpoint)).\n* Fix a bug that prevented interruption of a running [OpenAI-Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver). by `Ctrl+C`.\n* Add support the chunked prefill feature in `furiosa-llm` (see [Long-context Length and Chunked Prefill](../furiosa_llm/build-artifacts-by-examples.html#chunkedprefill)).\n* Enable direct building of bfloat16/float16/float32 models without quantization step (see [Building Float16, Float32 Models](../furiosa_llm/build-artifacts-by-examples.html#autobfloat16cast)).\n* Add support for the reasoning model parser in [OpenAI-Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) (see [Reasoning Support](../furiosa_llm/furiosa-llm-serve.html#reasoning)).\n* `LLM` API, `furiosa-mlperf`, `furiosa-llm serve` now support loading artifacts from Hugging Face Hub.\n* Add support for `npu_queue_limit` option in `furiosa-llm serve` command to configure the NPU queue limit.\n* `furiosa-llm` now supports Python 3.11 and 3.12.\n* Optimize the NPU DRAM stack usage for the `furiosa-llm`.\n* Support Ubuntu 24.04 (Noble Numbat).\n* Remove the group `furiosa` to access NPU devices on Linux system.\n* Pre-compiled model artifacts are now available in Hugging Face Hub.\n* Add support for `abort()` in `LLMEngine` and `AsyncLLMEngine` APIs.\n* Add support for the metrics endpoint (`/metrics`) used to monitor the health of [OpenAI-Compatible Server](../furiosa_llm/furiosa-llm-serve.html#openaiserver) (see [Metrics Endpoint](../furiosa_llm/furiosa-llm-serve.html#metricsendpoint)).\n* Support sampling parameter ‚Äúlogprobs‚Äù in Furiosa-LLM (see [SamplingParams class](../furiosa_llm/reference/sampling_params.html#samplingparams)).\n* Add support for Container Device Interface (CDI) for container runtimes (e.g., docker, containerd, and crio) (see [Container Support](../cloud_native_toolkit/container.html#container)).\n\n### üö® Breaking Changes[#](#id8 \"Link to this heading\")\n\n* SDK 2025.2.0 cannot load artifacts built with 2025.1.x. Please use an artifact built with 2025.2.x,\n  or rebuild the model again with the new SDK.\n* The furiosa group is no longer required to access NPU devices on Linux systems.\n\nCompoent Versions:\n\n| Package name | Version |\n| --- | --- |\n| furiosa-compiler | 2025.2.0 |\n| furiosa-driver-rngd | 2025.2.0 |\n| furiosa-firmware-tools-rngd | 2025.2.0 |\n| furiosa-firmware-image-rngd | 2025.2.0 |\n| furiosa-pert-rngd | 2025.2.0 |\n| furiosa-model-compressor | 2025.2.0 |\n| furiosa-llm | 2025.2.0 |\n| furiosa-llm-models | 2025.2.0 |\n| furiosa-mlperf | 2025.2.0 |\n| furiosa-mlperf-resources | 4.1.0 |\n| furiosa-native-compiler | 2025.2.0 |\n| furiosa-native-runtime | 2025.2.0 |\n| furiosa-feature-discovery | 2025.2.0 |\n| furiosa-device-plugin | 2025.2.0 |\n| furiosa-smi | 2025.2.0 |\n| furiosa-libsmi | 2025.2.0 |\n\n---\n\n## Furiosa SDK 2025.1.0 Beta1 (2025-02-24)[#](#furiosa-sdk-2025-1-0-beta1-2025-02-24 \"Link to this heading\")\n\n2025.1.0 is the third major SDK release for RNGD. This release includes a lot of new features and significant\nimprovements, including significant LLM latency optimization, tool-calling support in Furiosa-LLM,\nthe device remapping support for container environment, command line tools improvements, and bug fixes.\n\nPlease refer to the [Upgrading FuriosaAI‚Äôs Software](../get_started/upgrade_guide.html#upgradeguide) section for instructions on\nobtaining this update.\n\n### üöÄ Highlights[#](#release2025-1-0-highlights \"Link to this heading\")\n\n* LLM Latency Optimization (Up to 11.66% TTFT, 11.45% TPOT improvement for 30k inputs, 1k outputs)\n* Support Tool-calling in Furiosa-LLM ([Tool Calling](https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/furiosa-llm-serve.html#tool-calling))\n* Support Device remapping (e.g., `/dev/rngd/npu2pe0-3` -> `/dev/rngd/npu0pe0-3`) for container\n* Add the new command line tool `furiosa-llm build` to build easily an artifact from Hugging Face model ([Building a Model Artifact](https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/model-preparation-workflow.html#building-a-model-artifact))\n* Fix continuous batch scheduling bugs which occur in certain ranges of sequence lengths and batch sizes\n* Automatic configuration of the maximum KV-cache memory allocation\n* Reduce fragmentation in runtime memory allocation\n* Allow `furiosa-mlperf` command to specify `pipeline_parallel_size` and `data_parallel_size`\n* Add `--allowed-origins` argument to `furiosa-llm serve` ([OpenAIServer](https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/furiosa-llm-serve.html))\n* Fix `trust_remote_code` support bug in furiosa-llm\n* Support Min-p sampling in `SamplingParams` ([SamplingParams class](https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/reference/sampling_params.html))\n* Allow `npu:X` in addition to `npu:X:*` in `devices` option\n  :   + e.g., `furiosa-llm serve ./model --devices \"npu:0\"`\n* `furiosa-mlperf` command supports `npu_queue_limit`, `spare_blocks_ratio`, allowing to optimize the performance\n\n### ‚ö†Ô∏è Deprecations & Upcoming Changes[#](#deprecations-upcoming-changes \"Link to this heading\")\n\n* `LLM.from_artifacts()` API will be deprecated from the 2025.2.0 release. Please use `LLM.load_artifact()` instead ([LLM class](https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/reference/llm.html)).\n\n### üö® Breaking Changes[#](#id10 \"Link to this heading\")\n\n* `--model` option of `furiosa-llm serve` become a positional argument.\n  Please use `furiosa-llm serve <model>` instead of `furiosa-llm serve --model <model>`. ([OpenAIServer](https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/furiosa-llm-serve.html))\n\nVersions of components:\n\n| Package name | Version |\n| --- | --- |\n| furiosa-compiler | 2025.1.0 |\n| furiosa-driver-rngd | 2025.1.0 |\n| furiosa-firmware-tools-rngd | 2025.1.0 |\n| furiosa-firmware-image-rngd | 2025.1.0 |\n| furiosa-pert-rngd | 2025.1.0 |\n| furiosa-model-compressor | 2025.1.0 |\n| furiosa-llm | 2025.1.0 |\n| furiosa-llm-models | 2025.1.0 |\n| furiosa-mlperf | 2025.1.0 |\n| furiosa-mlperf-resources | 4.1.0 |\n| furiosa-native-compiler | 2025.1.0 |\n| furiosa-native-runtime | 2025.1.0 |\n| furiosa-feature-discovery | 2025.1.0 |\n| furiosa-device-plugin | 2025.1.0 |\n| furiosa-smi | 2025.1.0 |\n| furiosa-libsmi | 2025.1.0 |\n\n---\n\n## Furiosa SDK 2024.2.1 Beta0 (2025-01-10)[#](#furiosa-sdk-2024-2-1-beta0-2025-01-10 \"Link to this heading\")\n\n2024.2.1 is a minor release based on the 2024.2.0 major release.\n\nPlease refer to the [Upgrading FuriosaAI‚Äôs Software](../get_started/upgrade_guide.html#upgradeguide) section for instructions on\nobtaining this update.\n\n### üöÄ Highlights[#](#release2024-2-1-highlights \"Link to this heading\")\n\n* Support for context lengths of up to 32K in furiosa-llm for various models, including Llama 3.1, and EXAONE\n* Artifacts with the same `tensor_parallel_size` are compatible even with any `pipeline_parallel_size`\n\nComponent Versions:\n\n| Package name | Version |\n| --- | --- |\n| furiosa-compiler | 2024.2.0 |\n| furiosa-driver-rngd | 2024.2.1 |\n| furiosa-firmware-tools-rngd | 2024.2.1 |\n| furiosa-firmware-image-rngd | 2024.2.0 |\n| furiosa-pert-rngd | 2024.2.1 |\n| furiosa-model-compressor | 2024.2.0 |\n| furiosa-llm | 2024.2.1 |\n| furiosa-llm-models | 2024.2.0 |\n| furiosa-mlperf | 2024.2.1 |\n| furiosa-mlperf-resources | 4.1.0 |\n| furiosa-native-compiler | 2024.2.0 |\n| furiosa-native-runtime | 2024.2.1 |\n| furiosa-feature-discovery | 2024.2.0 |\n| furiosa-device-plugin | 2024.2.0 |\n| furiosa-smi | 2024.2.0 |\n| furiosa-libsmi | 2024.2.0 |\n\n---\n\n## Furiosa SDK 2024.2.0 Beta0 (2024-12-23)[#](#furiosa-sdk-2024-2-0-beta0-2024-12-23 \"Link to this heading\")\n\n2024.2.0 is the second major SDK release for RNGD.\nThis release includes many new features and significant improvements,\nincluding new model support, 8K context length,\nTensor Parallelism, PyTorch 2.4 support, Optimum API, and\nmultiple performance improvements.\n\nPlease refer to the [Upgrading FuriosaAI‚Äôs Software](../get_started/upgrade_guide.html#upgradeguide) section for instructions on\nobtaining this update.\n\n### üöÄ Highlights[#](#release2024-2-0-highlights \"Link to this heading\")\n\n* New model support: Solar, EXAONE-3.0, CodeLLaMA2, and Vicuna\n* Up to 8K context length support in models, such as Llama 3.1\n* Tensor Parallelism support (`tensor_parallel_size <= 8`)\n* PyTorch 2.4.1 support\n* Transformers 4.44.2 support\n* Furiosa-LLM\n  :   + ArtifactBuilder API and CLI tools (refer to [ArtifactBuilder](https://developer.furiosa.ai/v2024.2.0/en/furiosa_llm/furiosa-llm-build.html#artifactbuilder))\n        :   - Users can build artifacts from Hugging Face Hub models with Hugging Face Transformers compatible API\n      + Hugging Face Transformers compatible API support (furiosa\\_llm.optimum)\n        :   - AutoModel, AutoModelForCausalLM, AutoModelForQuestionAnswering API\n            - QuantizerForCausalLM API support for calibration and quantization\n      + LLMEngine, AsyncLLMEngine API support compatible with vLLM\n* About 20% performance improvements in models based on LlamaForCausalLM\n  :   + e.g., 3580 tokens/sec in Llama 3.1 8B model with a single RNGD card\n\n### üö® Breaking Changes[#](#id14 \"Link to this heading\")\n\n* LLM.from\\_artifacts() API has been deprecated. Please use LLM.load\\_artifacts() instead.\n* SDK 2024.2.x cannot load artifacts built with 2024.1.x. Please use an artifact built with 2024.2.x,\n  or rebuild the model again with the new SDK.\n\nComponent version[#](#id16 \"Link to this table\")\n\n\n\n\n| Package name | Version |\n| --- | --- |\n| furiosa-compiler | 2024.2.0 |\n| furiosa-driver-rngd | 2024.2.0 |\n| furiosa-firmware-tools-rngd | 2024.2.0 |\n| furiosa-firmware-image-rngd | 2024.2.0 |\n| furiosa-pert-rngd | 2024.2.0 |\n| furiosa-llm | 2024.2.0 |\n| furiosa-llm-models | 2024.2.0 |\n| furiosa-mlperf | 2024.2.0 |\n| furiosa-mlperf-resources | 4.1.0 |\n| furiosa-model-compressor | 2024.2.0 |\n| furiosa-native-compiler | 2024.2.0 |\n| furiosa-native-runtime | 2024.2.0 |\n| furiosa-smi | 2024.2.0 |\n| furiosa-libsmi | 2024.2.0 |\n| furiosa-device-plugin | 2024.2.0 |\n| furiosa-feature-discovery | 2024.2.0 |\n\n---\n\n## Furiosa SDK 2024.1.0 Alpha (2024-10-11)[#](#furiosa-sdk-2024-1-0-alpha-2024-10-11 \"Link to this heading\")\n\n2024.1.0 is the first SDK release for RNGD. This release is an alpha release,\nand the features and APIs described in this document may change in the future.\n\n### üöÄ Highlights[#](#release2024-1-0-highlights \"Link to this heading\")\n\n* Model Support: Llama 3.1 8B/70B, BERT Large, GPT-J 6B\n* Furiosa Quantizer supports the following quantization methods:\n  :   + BF16 (W16A16)\n      + INT8 Weight-Only (W8A16)\n      + FP8 (W8A8)\n      + INT8 SmoothQuant (W8A8)\n* Furiosa-LLM\n  :   + Efficient KV cache management with PagedAttention\n      + Continuous batching support in serving\n      + OpenAI-compatible API server\n      + Greedy search and beam search\n      + Pipeline Parallelism and Data Parallelism across multiple NPUs\n* `furiosa-mlperf` command\n  :   + Server and Offline scenarios\n      + BERT, GPT-J, LLaMA 3.1 benchmarks\n* System Management Interface\n  :   + System Management Interface Library and CLI for Furiosa NPU family\n* Cloud Native Toolkit\n  :   + Kubernetes integration for managing and monitoring the Furiosa NPU family\n\nComponent version[#](#id17 \"Link to this table\")\n\n\n\n\n| Package name | Version |\n| --- | --- |\n| furiosa-compiler | 2024.2.0 |\n| furiosa-device-plugin | 2024.2.0 |\n| furiosa-driver-rngd | 2024.2.0 |\n| furiosa-feature-discovery | 2024.1.0 |\n| furiosa-firmware-tools-rngd | 2024.1.0 |\n| furiosa-firmware-image-rngd | 2024.1.0 |\n| furiosa-libsmi | 2024.2.0 |\n| furiosa-llm | 2024.2.0 |\n| furiosa-llm-models | 2024.2.0 |\n| furiosa-mlperf | 2024.2.0 |\n| furiosa-mlperf-resources | 4.1.0 |\n| furiosa-model-compressor | 2024.1.0 |\n| furiosa-native-compiler | 2024.2.0 |\n| furiosa-native-runtime | 2024.2.0 |\n| furiosa-smi | 2024.1.0 |",
  "external_links": [
    {
      "text": "Qwen2.5-32B-Instruct",
      "url": "https://huggingface.co/furiosa-ai/Qwen2.5-32B-Instruct"
    },
    {
      "text": "Qwen2.5-Coder-14B-Instruct",
      "url": "https://huggingface.co/furiosa-ai/Qwen2.5-Coder-14B-Instruct"
    },
    {
      "text": "Qwen2.5-Coder-32B-Instruct",
      "url": "https://huggingface.co/furiosa-ai/Qwen2.5-Coder-32B-Instruct"
    },
    {
      "text": "Llama-3.3-70B-Instruct-INT8",
      "url": "https://huggingface.co/furiosa-ai/Llama-3.3-70B-Instruct-INT8"
    },
    {
      "text": "Hugging Face Hub",
      "url": "https://huggingface.co/furiosa-ai"
    },
    {
      "text": "EXAONE-3.5-32B-Instruct",
      "url": "https://huggingface.co/furiosa-ai/EXAONE-3.5-32B-Instruct"
    },
    {
      "text": "DeepSeek-R1-Distill-Llama-70B",
      "url": "https://huggingface.co/furiosa-ai/DeepSeek-R1-Distill-Llama-70B"
    },
    {
      "text": "Llama-3.3-70B-Instruct",
      "url": "https://huggingface.co/furiosa-ai/Llama-3.3-70B-Instruct"
    },
    {
      "text": "Llama-3.3-70B-Instruct-INT8",
      "url": "https://huggingface.co/furiosa-ai/Llama-3.3-70B-Instruct-INT8"
    },
    {
      "text": "Qwen2.5-Coder-32B-Instruct",
      "url": "https://huggingface.co/furiosa-ai/Qwen2.5-Coder-32B-Instruct"
    },
    {
      "text": "Hugging Face Hub",
      "url": "https://huggingface.co/furiosa-ai"
    },
    {
      "text": "vLLM benchmark",
      "url": "https://github.com/vllm-project/vllm/tree/main/benchmarks"
    },
    {
      "text": "LLMPerf",
      "url": "https://github.com/ray-project/llmperf"
    },
    {
      "text": "stream_options.include_usage",
      "url": "https://community.openai.com/t/usage-stats-now-available-when-using-streaming-with-the-chat-completions-api-or-completions-api/738156"
    },
    {
      "text": "Tool Calling",
      "url": "https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/furiosa-llm-serve.html#tool-calling"
    },
    {
      "text": "Building a Model Artifact",
      "url": "https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/model-preparation-workflow.html#building-a-model-artifact"
    },
    {
      "text": "OpenAIServer",
      "url": "https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/furiosa-llm-serve.html"
    },
    {
      "text": "SamplingParams class",
      "url": "https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/reference/sampling_params.html"
    },
    {
      "text": "LLM class",
      "url": "https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/reference/llm.html"
    },
    {
      "text": "OpenAIServer",
      "url": "https://developer.furiosa.ai/v2025.1.0/en/furiosa_llm/furiosa-llm-serve.html"
    },
    {
      "text": "ArtifactBuilder",
      "url": "https://developer.furiosa.ai/v2024.2.0/en/furiosa_llm/furiosa-llm-build.html#artifactbuilder"
    }
  ]
}