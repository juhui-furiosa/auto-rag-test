{
  "title": "FuriosaAI’s Software Stack",
  "url": "https://developer.furiosa.ai/latest/en/overview/software_stack.html",
  "version": "2025.3.1",
  "content": "# FuriosaAI’s Software Stack[#](#furiosaai-s-software-stack \"Link to this heading\")\n\nFuriosaAI offers a streamlined software stack that enables the FuriosaAI NPU to\nbe used across various applications and environments.\nHere, we outline the software stack provided by FuriosaAI, explaining\nthe roles of each component, along with guidelines and tutorials.\nThe following diagram shows the software stack provided by FuriosaAI.\n\n[![FuriosaAI Software Stack](../_images/sw_stack.png)](../_images/sw_stack.png)\n\n## Kernel Driver, Firmware, and PE Runtime[#](#kernel-driver-firmware-and-pe-runtime \"Link to this heading\")\n\nThe kernel device driver enables the Linux operating system to recognize NPU devices and\nexpose them as Linux device files.\nThe firmware runs on the NPU and provides low-level APIs to the PE Runtime\n(PERT) that runs on the Processing Element (PE).\nPERT is responsible for communicating with the host’s runtime, as well as\nscheduling and managing the resources of PEs to execute NPU tasks.\n\n## Furiosa Compiler[#](#furiosa-compiler \"Link to this heading\")\n\nThe Furiosa Compiler optimizes model graphs and generates executable programs\nfor the NPU.\nIt performs several optimizations, including graph-level optimizations, operator\nfusion, optimization of memory allocations, scheduling, and optimization of\ncross-layer data movements.\nA single model can be compiled into multiple executables, depending on\nthe model’s architecture and application requirements.\n\nWhen using FuriosaAI’s backend for `torch.compile()` (`FuriosaBackend`), or\nthe `furiosa-llm` package, the Furiosa Compiler is used transparently to\ngenerate NPU executables.\n\n## Furiosa Runtime[#](#furiosa-runtime \"Link to this heading\")\n\nThe Runtime loads the executables generated by the Furiosa compiler and runs\nthem on the NPU.\nThe Runtime is responsible for scheduling NPU programs and allocating memory\non both the NPUs and the host RAM.\nAdditionally, the Runtime supports the use of multiple NPUs and provides a\nunified entry point for running models across multiple NPUs seamlessly.\n\n## Furiosa Model Compressor (Quantizer)[#](#furiosa-model-compressor-quantizer \"Link to this heading\")\n\nThe Furiosa Model Compressor is a toolkit for model calibration and quantization.\nModel quantization is a powerful technique to reduce memory usage, computation\ncost, inference latency, and power consumption.\nThe Furiosa Model Compressor provides post-training quantization methods, such as:\n\n* BF16 (W16A16)\n* INT8 Weight-Only (W8A16) (Planned)\n* FP8 (W8A8)\n* INT8 SmoothQuant (W8A8) (Planned)\n* INT4 Weight-Only (W4A16 AWQ / GPTQ) (Planned)\n\n## Furiosa-LLM[#](#furiosa-llm \"Link to this heading\")\n\nFuriosa-LLM is a high-performance inference engine for LLM models, such as Llama and GPT-J.\nThe key features of Furiosa-LLM include:\n\n* vLLM-compatible API, for seamless integration with vLLM-based workflows;\n* PagedAttention, for optimized memory usage for attention computation;\n* Continuous batching, improving throughput by dynamically grouping inference requests;\n* [Hugging Face Hub](https://huggingface.co/docs/huggingface_hub) support,\n  simplifying access to pre-trained models;\n* OpenAI-compatible API server, enabling easy deployment using familiar APIs.\n\nFor more information, please refer to the [Furiosa-LLM](../furiosa_llm/intro.html#furiosallm) section.\n\n## Kubernetes Support[#](#kubernetes-support \"Link to this heading\")\n\nKubernetes, an open-source platform for managing containerized applications\nand services, is widely adopted by organizations for its powerful capabilities\nin deploying, scaling, and automating containerized workloads.\nThe FuriosaAI software stack offers native integration with Kubernetes,\nenabling seamless deployment and management of AI applications within\nKubernetes environments.\nFuriosaAI’s device plugin enables Kubernetes clusters to recognize FuriosaAI’s\nNPUs and schedule them for workloads that require them.\nThis integration simplifies the deployment of AI workloads with FuriosaAI NPUs,\nensuring efficient resource utilization and scalability.\n\nFor more information about Kubernetes support, please refer to\nthe [Cloud Native Toolkit](../cloud_native_toolkit/intro.html#cloudnativetoolkit) section.",
  "external_links": [
    {
      "text": "Hugging Face Hub",
      "url": "https://huggingface.co/docs/huggingface_hub"
    }
  ]
}