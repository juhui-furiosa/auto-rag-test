{
  "title": "Kubernetes Plugins",
  "url": "https://developer.furiosa.ai/latest/en/cloud_native_toolkit/kubernetes.html",
  "version": "2025.3.1",
  "content": "# Kubernetes Plugins[#](#kubernetes-plugins \"Link to this heading\")\n\n## Kubernetes Support[#](#kubernetes-support \"Link to this heading\")\n\nWe do support the following versions of Kubernetes and CRI runtime:\n\n* Kubernetes: v1.24.0 or later\n* helm v3.0.0 or later\n* CRI Runtime: [containerd](https://github.com/containerd/containerd) or [CRI-O](https://github.com/cri-o/cri-o)\n\nNote\n\nDocker is officially deprecated as a container runtime in Kubernetes.\nIt is recommended to use containerd or CRI-O as a container runtime.\nOtherwise you may face unexpected issues with NPU plugins.\nFor more information, see [here](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/).\n\n## NPU Plugins Deployment Guide[#](#npu-plugins-deployment-guide \"Link to this heading\")\n\nAs an administrator, you have to install [prerequisites](../get_started/prerequisites.html#installingprerequisites) such as driver, firmware on NPU nodes.\nIf prerequisites are not installed, the NPU plugins will return an error.\n\nWe recommend to deploy NPU plugins using Helm charts. Since it is difficult to control deployment of each plugin individually.\nIf NPU plugins are deployed on non-NPU nodes, the plugins will return not supported errors and the pods will be in a CrashLoopBackOff state.\n\nThe helm chart deploy the Node Feature Discovery (NFD) by default.\nOnce NFD is deployed, NPU nodes are automatically labelled with PCI IDs and capabilities.\nHere is an example of the labels that are attached to the NPU node:\n\n```\nfeature.node.kubernetes.io/pci-1200_1ed2.present: \"true\"\nfeature.node.kubernetes.io/pci-1200_1ed2.sriov.capable: \"true\"\n```\n\nCopy to clipboard\n\nThe NPU Plugins helm chart set nodeAffinity like below to control the deployment of NPU plugins only on NPU nodes.\n\n```\naffinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: feature.node.kubernetes.io/pci-1200_1ed2.present\n          operator: In\n          values:\n            - \"true\"\n```\n\nCopy to clipboard\n\nNote\n\nIf you already have NFD deployed in your cluster, you can configure helm-chart to skip NFD deployment.\nHowever, you need to ensure that NFD has default label policy. Default label policy includes PCI class and Vendor ID fields in the label.\n\n```\nfeature.node.kubernetes.io/pci_<CLASS_ID>_<VENDOR_ID>.present\n```\n\nCopy to clipboard\n\nIn some cases, NFD is configured to omit PCI class field in the label.\n\n```\nfeature.node.kubernetes.io/pci_<VENDOR_ID>.present\n```\n\nCopy to clipboard\n\nYou have two options to resolve this issue: either deploy two NFD instances in the cluster with different label policies,\nmanaging them using taint and toleration,or modify the NPU plugins Helm chart to use the correct label.\n\nSee the following links for more information and configurations of each plugin:\n\n* [Installing Furiosa Feature Discovery](kubernetes/feature_discovery.html)\n* [Installing Furiosa Device Plugin](kubernetes/device_plugin.html)\n* [Installing Furiosa Metrics Exporter](kubernetes/metrics_exporter.html)\n\nOnce you have installed the NPU plugins, your cluster exposes Furiosa NPUs as schedulable resources, such as `furiosa.ai/rngd`.\n\nTo ensure your node is ready, you can examine Capacity and/or Allocatable field of `v1.node` object.\nHere is an example of node that has 2 RNGD NPUs:\n\n```\n...\nstatus:\n  ...\n  allocatable:\n    cpu: \"20\"\n    ephemeral-storage: \"1770585791219\"\n    furiosa.ai/rngd: \"2\"\n    hugepages-1Gi: \"0\"\n    hugepages-2Mi: \"0\"\n    memory: 527727860Ki\n    pods: \"110\"\n  capacity:\n    cpu: \"20\"\n    ephemeral-storage: 1921208544Ki\n    furiosa.ai/rngd: \"2\"\n    hugepages-1Gi: \"0\"\n    hugepages-2Mi: \"0\"\n    memory: 527830260Ki\n    pods: \"110\"\n...\n```\n\nCopy to clipboard\n\nThe following command should show the `Capacity` field of each node in the Kubernetes cluster.\n\n```\nkubectl get nodes -o json | jq -r '.items[] | .metadata.name as $name | .status.capacity | to_entries | map(\"    \\(.key): \\(.value)\") | $name + \":\\n  capacity:\\n\" + join(\"\\n\")'\n```\n\nCopy to clipboard\n\n## Requesting NPUs[#](#requesting-npus \"Link to this heading\")\n\nYou can consume NPUs from your containers in a Pod by requesting NPU resources, the same way you request CPU or memory.\n\nHowever, since NPUs are exposed as a custom resource, there are some limitations you should be aware of when requesting NPU resources:\n\n* You can specify NPU `limits` without specifying `requests`, because kubernetes will use limit as request if request is not specified.\n* You can specify NPU in both `limits` and `requests` but these two values must be equal.\n* You cannot specify NPU `request` without specifying `limits`.\n\nHere is an example manifest for a Pod that requests 2 RNGD NPUs:\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-npu-request\nspec:\n  containers:\n  - name: furiosa\n    image: furiosaai/furiosa-smi:latest\n    imagePullPolicy: IfNotPresent\n    command: [\"sleep\"]\n    args: [\"120\"]\n    resources:\n      limits:\n        furiosa.ai/rngd: 2\n```\n\nCopy to clipboard\n\n## Scheduling NPUs With Specific Requirements[#](#scheduling-npus-with-specific-requirements \"Link to this heading\")\n\nIn certain cases, user may need to schedule NPU workload on node that meet specific hardware or software requirements, such as particular driver versions.\nIf the [Furiosa Feature Discovery](kubernetes/feature_discovery.html#featurediscovery) is deployed in your cluster, nodes are automatically labelled based on their hardware and software configurations including driver version.\nThis allows user to schedule Pod on nodes that meet specific requirements.\n\nFollowing example shows how to use affinity to schedule a Pod that request 2 RNGD NPUs with specific driver version:\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-npu-scheduling-with-affinity\nspec:\n  containers:\n  - name: furiosa\n    image: furiosaai/furiosa-smi:latest\n    imagePullPolicy: IfNotPresent\n    command: [\"sleep\"]\n    args: [\"120\"]\n    resources:\n      limits:\n        furiosa.ai/rngd: 2\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: furiosa.ai/driver-version\n            operator: In\n            values:\n            - \"1.0.12\"\n```\n\nCopy to clipboard\n\n## Known Issues[#](#known-issues \"Link to this heading\")\n\n* A pod that uses an external device, like an NPU, may not restart and will end up in an UnexpectedAdmissionError when the node is restarted. This happens because the kubelet has a race condition when allocating external resources to the restarting pod. The issue is tracked in [kubernetes/kubernetes#128043](https://github.com/kubernetes/kubernetes/issues/128043)",
  "external_links": [
    {
      "text": "containerd",
      "url": "https://github.com/containerd/containerd"
    },
    {
      "text": "CRI-O",
      "url": "https://github.com/cri-o/cri-o"
    },
    {
      "text": "here",
      "url": "https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/"
    },
    {
      "text": "kubernetes/kubernetes#128043",
      "url": "https://github.com/kubernetes/kubernetes/issues/128043"
    }
  ]
}