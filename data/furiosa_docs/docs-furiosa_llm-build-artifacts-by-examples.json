{
  "title": "Building Model Artifacts By Examples",
  "url": "https://developer.furiosa.ai/latest/en/furiosa_llm/build-artifacts-by-examples.html",
  "version": "2025.3.1",
  "content": "# Building Model Artifacts By Examples[#](#building-model-artifacts-by-examples \"Link to this heading\")\n\nThis document provides examples of how to build model artifacts using Furiosa-LLM.\nIf you’re unfamiliar with the concept of model artifacts or the overall workflow for preparing a model for Furiosa-LLM,\nplease refer to the [Model Preparation](model-preparation.html#modelpreparation) section for an introduction.\n\n## Prerequisites[#](#prerequisites \"Link to this heading\")\n\nBefore starting this section, please ensure you have the following prerequisites:\n\n* [Setting up APT](../get_started/prerequisites.html#aptsetup) and [Installing Prerequisites](../get_started/prerequisites.html#installingprerequisites)\n* [Furiosa-LLM Installation](../get_started/furiosa_llm.html#installingfuriosallm)\n* [HuggingFace Access Token](../get_started/furiosa_llm.html#authorizinghuggingfacehub)\n\n## Command-Line Tool vs. ArtifactBuilder API[#](#command-line-tool-vs-artifactbuilder-api \"Link to this heading\")\n\nThere are two ways to build model artifacts using Furiosa-LLM:\n\n* The [ArtifactBuilder](reference/artifact_builder.html#artifactbuilderclass) API\n* The `furiosa-llm build` command-line tool\n\nThe [ArtifactBuilder](reference/artifact_builder.html#artifactbuilderclass) API is a Python interface that enables you to build model artifacts\nwith Furiosa-LLM, offering greater programmability and flexibility.\n\nThe `furiosa-llm build` command is a command-line tool that provides the same functionality\nas the [ArtifactBuilder](reference/artifact_builder.html#artifactbuilderclass) API. It offers a simpler and more convenient way to build model artifacts\nwithout writing any code. Its syntax is as follows:\n\n```\nfuriosa-llm build <MODEL_ID_OR_PATH> <OUTPUT_PATH> [OPTIONS]\n```\n\nCopy to clipboard\n\nYou can see more options by running `furiosa-llm build --help`.\nEssentially, it is a wrapper around the `ArtifactBuilder` API,\nso you can find details about the options in the reference for the [ArtifactBuilder](reference/artifact_builder.html#artifactbuilderclass) API.\n\n## Examples[#](#examples \"Link to this heading\")\n\nThe examples in this section are based on `furiosa-llm build` command and the Llama 3.1 8B model.\nYou can also implement the same examples using the [ArtifactBuilder](reference/artifact_builder.html#artifactbuilderclass) API.\n\n### Basic Examples[#](#basic-examples \"Link to this heading\")\n\nBelow is a basic example of building a model artifact using the `furiosa-llm build` command.\nThe first argument can be a Hugging Face model ID or a local path.\nA local path should start with `.` or `/` and should point to a directory\ncontaining a Hugging Face Transformers model or a quantized model by Furiosa-LLM.\nThe second argument is the output path where the model artifact will be saved.\n\nThe following command doesn’t have any options, so the model artifact will be built with the default options.\nYou can find the default options by running `furiosa-llm build --help`.\n\n```\nfuriosa-llm build meta-llama/Llama-3.1-8B-Instruct \\\n    ./Output-Llama-3.1-8B-Instruct\n```\n\nCopy to clipboard\n\n### Specifying Context Length[#](#specifying-context-length \"Link to this heading\")\n\nThe `--max-seq-len-to-capture` option specifies the maximum sequence length supported by the Furiosa-LLM engine.\nYou can adjust this value according to your model and use case, as it affects how the model is optimized.\n\n```\nfuriosa-llm build meta-llama/Llama-3.1-8B-Instruct \\\n    ./Output-Llama-3.1-8B-Instruct \\\n    --max-seq-len-to-capture 4096\n```\n\nCopy to clipboard\n\n### Specifying Buckets[#](#specifying-buckets \"Link to this heading\")\n\nFuriosa-LLM utilizes buckets to optimize models for varying sequence lengths.\nThe `--prefill-buckets BUCKET_SIZE` and `--decode-buckets BUCKET_SIZE`\noptions specify a comma-separated list of bucket sizes for prefill and decode phases, respectively.\n`-pb` and `-db` are the short options for `--prefill-buckets` and `--decode-buckets`, respectively.\nThe prefill bucket size is used during the input prompt processing phase, while the decode bucket size is used during the text-generation phase.\nEach bucket is defined as a tuple of batch size and sequence length, specified as `BATCH_SIZE,SEQ_LEN`.\nYou can specify multiple bucket sizes for the prefill and decode phases as follows:\n\n```\nfuriosa-llm build meta-llama/Llama-3.1-8B-Instruct \\\n    ./Output-Llama-3.1-8B-Instruct \\\n    -pb 1,512 \\\n    -pb 1,1024 \\\n    -db 4,1024 \\\n    -db 4,2048\n```\n\nCopy to clipboard\n\nThe above example specifies two prefill buckets: `1,512` and `1,1024`, and two decode buckets: `4,1024` and `4,2048`.\nThis means that the model can process up to 1024 tokens in input prompts in a single batch,\nand generate up to 2048 tokens in 4 batches.\n\nTip\n\nWe highly recommend using a batch size of 1 for prefill buckets. For decode buckets, use larger batch sizes to improve\nutilization—preferably powers of two, such as 2, 4, or 8. This is because the prefill phase typically saturates compute\ncapacity even with small batches, whereas the decode phase suffers from low compute efficiency, as it generates\nonly one token per request.\n\n### Using Parallelism for Large Models[#](#using-parallelism-for-large-models \"Link to this heading\")\n\nTo serve large models efficiently, model parallelism is required to distribute the model across multiple NPUs.\nFuriosa-LLM supports 3D parallelism, which includes tensor parallelism, pipeline parallelism, and data parallelism.\n\nThe `--tensor-parallel-size PE_NUM` option specifies the number of PEs (Processing Elements) to use for tensor parallelism.\nNote that we use the number of PEs as the unit of tensor parallelism because a single RNGD contains 8 partitionable PEs.\n\nThe degree of tensor parallelism must be a power of two, starting from 1 (e.g., 1, 2, 4, 8, etc.).\nWe recommend using 1, 4, or 8 as the tensor parallelism size.\nFor example, a value of 1 or 4 is recommended for BERT models, while 4 or 8 is recommended for Llama models.\n\n```\nfuriosa-llm build meta-llama/Llama-3.1-8B-Instruct \\\n    ./Output-Llama-3.1-8B-Instruct \\\n    --tensor-parallel-size 8\n```\n\nCopy to clipboard\n\nThe `--pipeline-parallel-size DEVICE_NUM` option specifies the number of devices across which the model should be partitioned for pipeline parallelism.\nFor example, if you have 4 devices and want to use all of them for pipeline parallelism,\nyou can specify `--pipeline-parallel-size 4`.\n\n```\nfuriosa-llm build Qwen/Qwen2.5-Coder-32B-Instruct \\\n    ./Output-Qwen2.5-Coder-32B-Instruct \\\n    --tensor-parallel-size 8 \\\n    --pipeline-parallel-size 4\n```\n\nCopy to clipboard\n\nNote\n\nNote that the `--pipeline-parallel-size` option can be overriden at model load time.\nTherefore, the settings specified at build time serve only as a preferred configuration.\nIn contrast, the `--tensor-parallel-size` option is fixed at build time and cannot be changed at load time.\n\nTo learn more about model parallelism, please refer to the [Model Parallelism](model-parallelism.html#modelparallelism) section.\n\n### Enabling `trust_remote_code`[#](#enabling-trust-remote-code \"Link to this heading\")\n\n`trust_remote_code` is an option in Hugging Face Transformers that allows you to trust remote code\nwhen loading a model from the Hugging Face Hub. You can use this option when building a model artifact\nfrom remote code on the Hugging Face Hub as follows:\n\n```\nfuriosa-llm build LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct \\\n    ./Output-EXAONE-3.5-7.8B-Instruct \\\n    --auto-bfloat16-cast \\\n    --trust-remote-code\n```\n\nCopy to clipboard\n\n### Building Float16, Float32 Models[#](#building-float16-float32-models \"Link to this heading\")\n\nFuriosa-LLM builds models in bfloat16 format by default. However, if you want to build a model\nin float16 or float32 format, you must explicitly request casting to bfloat16\nusing the `--auto-bfloat16-cast` flag; otherwise, an error will occur.\nThe example in [Enabling trust\\_remote\\_code](#trustremotecode) uses `--auto-bfloat16-cast` option because the EXAONE model uses float16 as dtype.\n\n### Long-context Length and Chunked Prefill[#](#long-context-length-and-chunked-prefill \"Link to this heading\")\n\nChunked prefill is a technique that splits large prefills into small chunks.\nThis is an experimental feature and is still under development.\nIt does not yet support batching a single prefill with multiple decode requests.\nHowever, it can still be useful for long-context length models.\nTo enable this feature, you need to specify the `--prefill-chunk-size CHUNK_SIZE` option as follows:\n\n```\nfuriosa-llm build LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct \\\n    ./Output-EXAONE-3.5-7.8B-Instruct \\\n    -tp 8 \\\n    --max-seq-len-to-capture 32768 \\\n    --prefill-chunk-size 8192 \\\n    -db 4,32768 \\\n    --auto-bfloat16-cast \\\n    --trust-remote-code\n```\n\nCopy to clipboard\n\n### Building Quantized Models[#](#building-quantized-models \"Link to this heading\")\n\nYou can find an example of building a quantized model in the [Model Quantization (Optional)](model-preparation.html#modelquantization) section.\n\n### Building Model Artifacts in Parallel[#](#building-model-artifacts-in-parallel \"Link to this heading\")\n\nBuilding a large model artifact can take a long time.\nTo speed up the process, you can use multiple workers to build the model artifact.\nThe `--num-pipeline-builder-workers [NUM]` option specifies the number of\nworkers used to analyze and convert a model graph into multiple execution plans,\neach of which serves as input to the Furiosa compiler.\nThis option is especially useful when building multiple buckets for a model.\n\nNote\n\nThe `--num-pipeline-builder-workers NUM` option requires CPU memory\napproximately equal to NUM times the total size of the model weights.\nEnsure that you have sufficient CPU memory available before using this option.\n\nThe `--num-compile-workers NUM` option specifies the number of workers to use for compiling a model artifact.\nThis option can speed up the compilation process by distributing the compilation tasks across multiple workers.\n\nThe following example shows how to build a model artifact using 4 pipeline builder workers and 4 compile workers.\n\n```\nfuriosa-llm build meta-llama/Llama-3.1-8B-Instruct \\\n    ./Output-Llama-3.1-8B-Instruct \\\n    --tensor-parallel-size 8  \\\n    --num-pipeline-builder-workers 4 \\\n    --num-compile-workers 4\n```\n\nCopy to clipboard",
  "external_links": []
}