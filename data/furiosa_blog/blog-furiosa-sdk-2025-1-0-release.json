{
  "metadata": {
    "url": "https://furiosa.ai/blog/furiosa-sdk-2025-1-0-release",
    "title": "Furiosa releases Furiosa SDK 2025.1.0",
    "category": "Technical Updates",
    "date": "2025-02-27T00:00:00+01:00"
  },
  "content": "Weâ€™re excited to introduce Furiosa SDK 2025.1.0 ( release notes ), packed with major enhancements for seamless LLM deployment with our user-friendly Furiosa LLM APIs. This release includes significant latency optimizations, tool calling support, flexible device remapping in containers, and a streamlined process for converting Hugging Face models.\n\nThis marks our third SDK release for RNGD in just five months, reflecting our rapid innovation to deliver the most efficient LLM inference for data centers.\n\nLLM Latency Optimization\n\nFuriosa SDK 2025.1.0 delivers significant LLM latency optimizations, improving TTFT by up to 11.66% and TPOT by 11.45% for large inputs (30K tokens) and outputs (1K tokens).\n\nThis means faster response times and better efficiency for high-throughput AI workloads.\n\nGet started by updating to the latest SDK and benchmarking supported models .\n\nOpenAI APIâ€™s Tool Calling Support\n\nFuriosa LLM now supports tool calling, enabling models to interact with external tools and functions.\n\nThis allows developers to seamlessly integrate AI-driven automation into their applications with minimal changes, which is critical for building Agentic AI applications.\n\nGet started by using the llama3_json parser, with more options coming in future releases. For more information, refer to the tool calling documentation .\n\nfuriosa-llm build for easily converting Hugging Face models\n\nThe new furiosa-llm build command simplifies converting Hugging Face models into optimized model artifacts for RNGD.\n\nThis streamlines deployment, reducing manual setup while ensuring peak performance.\n\nGet started by referring to the building a model artifact documentation .\n\nAutomatic optimization of Blocked KV cache allocation\n\nFuriosa SDK 2025.1.0 automatically maximizes blocked KV cache allocation by reducing memory fragmentation,  improving memory efficiency for LLM inference.\n\nYou donâ€™t need to manually tune to get the best performance. With the latest release of Furiosa LLM engine, you automatically get maximum performance.\n\nGet started with Furiosa LLM with this quick start documentation .\n\nAnd thereâ€™s more coming! In the next few months, weâ€™re rolling out enhanced tensor parallelism, speculating with a draft model, embeddings API support, a torch.compile () backend, and more.\n\nWith RNGD now in key enterprise customersâ€™ hands, weâ€™re prioritizing rapid SDK updates, so join us on our journey.\n\nðŸ”— Sign up to be notified first about RNGD: https://furiosa.ai/signup ."
}