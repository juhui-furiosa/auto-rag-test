{
  "metadata": {
    "url": "https://furiosa.ai/blog/why-were-joining-the-uec-the-future-of-llm-inference-is-multi-chip",
    "title": "Why we’re joining the UEC: The future of LLM inference is multi-chip",
    "category": "News",
    "date": "2024-12-02T00:00:00+01:00"
  },
  "content": "The AI industry needs better ways to connect chips together. Model sizes are growing faster than individual chips can keep up, and proprietary interconnect solutions limit innovation. That's why we're excited to join the Ultra Ethernet Consortium (UEC) and help build open standards for chip-to-chip communication.\n\nAs part of the UEC, we will help the organization develop open standards for high-bandwidth, low-latency chip-to-chip communication – a key part of building scalable and sustainable AI infrastructure.\n\nWhy multi-chip AI is here to stay\n\nAlthough there has been exciting progress in building small language models, today’s leading LLMs and multimodal models exceed the memory capacity of most single chips, even those with large on-chip SRAM.\n\nWhile Llama 70B represents the current benchmark for LLMs, architectures like Mixtral 8x7B demonstrate even greater memory demands. With transistor density improvements plateauing and model sizes continuing to grow, multi-chip solutions are essential to scaling AI inference.\n\nParallelism is key\n\nMulti-chip deployments use several different parallelism strategies. Data parallelism distributes inference requests across chips; tensor parallelism splits individual model layers. Expert parallelism in MoE models like Mixtral routes computations to different specialized units.\n\nThe Tensor Contraction Processor architecture in FuriosaAI’s second-gen chip, RNGD (“Renegade”), excels at leveraging different forms of parallelism because it eliminates the need to  first break tensors down into matrices.\n\nFor all these approaches, high-bandwidth interconnect is crucial.\n\nThe power of open standards in AI\n\nOpen approaches have been key to accelerating progress in AI, from the public COCO dataset in 2013 right through to today’s Llama models. Hardware initiatives like the Open Compute Project and open memory standards like HBM3 and HBM3e have driven innovation through standardization and competition.\n\nUltra Ethernet will be a similarly important step forward for the industry. It offers several compelling advantages over current solutions. Ultra Ethernet’s vendor-neutral approach ensures broad compatibility, while delivering much greater bandwidth than PCIe.\n\nThe architecture also enables superior performance per watt in chip-to-chip communication, lowering the cost and complexity of inference at scale.\n\nIt can also enable mixing different vendors’ accelerators in single deployments and scaling easily from individual servers to large-scale multirack configurations.\n\nUltra Ethernet and Furiosa\n\nThis aligns perfectly with Furiosa’s approach to AI inference. RNGD’s TCP architecture easily scales across multiple chips, and our software stack is built for distributed deployments from day one.\n\nBy joining the UEC, we’re reinforcing our commitment to solutions that combine performance, power efficiency, and programmability in ways that easily scale to tomorrow’s models and use cases."
}