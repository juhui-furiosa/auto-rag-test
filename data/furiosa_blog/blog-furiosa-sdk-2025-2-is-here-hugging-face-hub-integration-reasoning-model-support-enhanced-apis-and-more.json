{
  "metadata": {
    "url": "https://furiosa.ai/blog/furiosa-sdk-2025-2-is-here-hugging-face-hub-integration-reasoning-model-support-enhanced-apis-and-more",
    "title": "Furiosa SDK 2025.2.0 is here: Hugging Face Hub integration, reasoning model support, enhanced APIs, and more",
    "category": "Technical Updates",
    "date": "2025-05-19T00:00:00+02:00"
  },
  "content": "Furiosa SDK 2025.2.0 is now available, bringing significant new functionality to RNGD, our flagship chip for power-efficient inference on LLMs and agentic AI.\n\nFuriosa SDK 2025.2.0 highlights include:\n\nSupport for reasoning parser and reasoning models, including DeepSeek-R1-Distill\n\nSupport for reasoning parser and reasoning models, including DeepSeek-R1-Distill\n\nBuilding of models in bfloat16, float32, and float16 formats directly from Hugging Face Hub without a separate quantization step\n\nBuilding of models in bfloat16, float32, and float16 formats directly from Hugging Face Hub without a separate quantization step\n\nSupport for new metadata endpoints /v1/models and /version\n\nSupport for new metadata endpoints /v1/models and /version\n\nSupport for Chat with tool in LLM API, supporting conversation with tool calling\n\nSupport for Chat with tool in LLM API, supporting conversation with tool calling\n\nA new metrics endpoint (/metrics)  for monitoring\n\nA new metrics endpoint (/metrics)  for monitoring\n\nSupport for chunked prefill to allow splitting large prefills into smaller chunks\n\nSupport for chunked prefill to allow splitting large prefills into smaller chunks\n\nSimplified Linux setup: NPU device access no longer requires joining the 'furiosa' group\n\nSimplified Linux setup: NPU device access no longer requires joining the 'furiosa' group\n\nUbuntu 24.04 (Noble Numbat) support\n\nUbuntu 24.04 (Noble Numbat) support\n\nPython 3.11 and Python 3.12 support\n\nPython 3.11 and Python 3.12 support\n\nThis is our fourth SDK release for RNGD in just six months, reflecting our commitment to rapid iteration for developers using RNGD for inference in data centers.\n\nStreamlined model access with Hugging Face Hub\n\nA major focus of this release is tighter integration with the Hugging Face ecosystem:\n\nThe LLM API, furiosa-mlperf, and furiosa-llm serve now support loading model artifacts directly from the Hugging Face Hub here .\n\nThe LLM API, furiosa-mlperf, and furiosa-llm serve now support loading model artifacts directly from the Hugging Face Hub here .\n\nPre-compiled model artifacts are also available on Hugging Face, so developers can use optimized models immediately.\n\nPre-compiled model artifacts are also available on Hugging Face, so developers can use optimized models immediately.\n\nModels can be accessed via commands like this:\n\nfuriosa-llm serve furiosa-ai/Llama-3.1-8B-Instruct\n\nLarger (70B) models are available via HTTPS:\n\nfuriosa-ai/DeepSeek-R1-Distill-Llama-70B ( download )\n\nfuriosa-ai/DeepSeek-R1-Distill-Llama-70B ( download )\n\nfuriosa-ai/Llama-2-70b-chat-hf-FP8 ( download )\n\nfuriosa-ai/Llama-2-70b-chat-hf-FP8 ( download )\n\nfuriosa-ai/Llama-3.3-70B-Instruct ( download )\n\nfuriosa-ai/Llama-3.3-70B-Instruct ( download )\n\nfuriosa-ai/Llama-3.3-70B-Instruct-FP8 ( download )\n\nfuriosa-ai/Llama-3.3-70B-Instruct-FP8 ( download )\n\nA much simpler way to build model artifacts\n\nBefore this release, building a model artifact required the calibration and quantization steps. The 2025.2 release allows a direct build of a bfloat16 model artifact without those steps. Additionally, if you specify --auto-bfloat16-cast, you can directly build float16 and float32 models too, by casting to bfloat16.\n\nReasoning model support\n\nFuriosa-LLM provides support for models with reasoning capabilities, such as the Deepseek R1 series. These models are designed to generate reasoning steps and then provide a final answer. For this mechanism, these models require a special parser to recognize the reasoning steps. To use the reasoning model, you need to specify --enable-reasoning and --reasoning-parser as follows:\n\nChunked prefill\n\nFuriosa-LLM now supports an experimental feature, ‚Äúchunked prefill,‚Äù that splits large prefills into smaller chunks. Chunked prefill is still under development and doesn‚Äôt yet batch a single prefill and multiple decode requests. However, it‚Äôs still useful when you have to handle a large context length.\n\nTo enable chunked prefill, add the --prefill-chunk-size [CHUNK_SIZE] option to the furiosa-llm build command. The following shows an example command for building the LG EXAONE model with a 32K context length.\n\nIf you are not familiar with furiosa-llm, please check out our quick start guide .\n\nEnhanced API functionality\n\nFuriosa LLM now offers enhanced APIs compatible with OpenAI standards. The Embedding API allows developers to generate high-quality embeddings.\n\nFuriosa LLM now offers enhanced APIs compatible with OpenAI standards. The Embedding API allows developers to generate high-quality embeddings.\n\nThe new Chat API, accessible via the LLM.chat() method, enables creation of dynamic, multiturn chat applications.\n\nThe new Chat API, accessible via the LLM.chat() method, enables creation of dynamic, multiturn chat applications.\n\nWe added /v1/models (and /v1/models/{model_id}) and /version endpoints to the OpenAI-compatible server for better introspection and management. A new /metrics endpoint allows for server monitoring.\n\nWe added /v1/models (and /v1/models/{model_id}) and /version endpoints to the OpenAI-compatible server for better introspection and management. A new /metrics endpoint allows for server monitoring.\n\nWe added support for abort() to the LLMEngine and AsyncLLMEngine APIs.\n\nWe added support for abort() to the LLMEngine and AsyncLLMEngine APIs.\n\nSupport for standard container runtimes\n\nThis release adds official support for industry-standard container runtimes: Docker v25.0.0 or later, ContainerD v1.7.0 or later, and CRI-O v1.28.0 or later.\n\nDocumentation is available here .\n\nMuch more to come\n\nFor our next SDK release, we plan to add additional Tensor Parallelism support for inter-chip communication and speculative decoding in Furiosa LLM. Stay tuned for more updates coming soon.\n\nüîóSign up to be notified first about RNGD at furiosa.ai/signup ."
}