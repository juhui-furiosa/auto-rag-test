{
  "name": "draft-based-approx-llm",
  "url": "https://github.com/furiosa-ai/draft-based-approx-llm",
  "visibility": "public",
  "readme": {
    "title": "Draft-based Approximate Inference for LLMs",
    "sections": [
      {
        "heading": "Draft-based Approximate Inference for LLMs",
        "text": "Kevin Galim\n1*\n,\nEthan Ewer\n2*\n,\nWonjun Kang\n1,3\n,\nMinjae Lee\n1\n,\nHyung Il Koo\n1,4\n,\nKangwook Lee\n2\n1\nFuriosaAI,\n2\nUW-Madison,\n3\nSeoul National University,\n4\nAjou University",
        "children": [
          {
            "heading": "üöÄ Overview",
            "text": "Draft-based Approximate Inference for LLMs\nleverages small draft models to more sharply distinguish important tokens and key-value (KV) pairs in long-context large language models. Our core contributions,\nSpecKV\nand\nSpecPC\n, enable smarter KV cache eviction and prompt compression, delivering more precise, efficient approximate inference than existing techniques.",
            "children": []
          },
          {
            "heading": "üìù Abstract",
            "text": "Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework:\nSpecKV\n, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping\nSpecPC\n, which uses the draft model's attention activations to identify and discard unimportant prompt tokens.\nTo the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput.",
            "children": []
          },
          {
            "heading": "üåü Features",
            "text": "Plug & Play\n: Add to any HuggingFace-compatible LLM with just a few lines.\nHigher Retained Accuracy\n: SpecKV/SpecPC preserve more model accuracy vs previous methods.\nFlexible\n: Supports Qwen2.5, Llama-3, and more.",
            "children": []
          },
          {
            "heading": "üõ†Ô∏è Installation",
            "text": "1. Clone repository:\ngit clone https://github.com/furiosa-ai/draft-based-approx-llm\n2. Install PyTorch (example for CUDA 12.4):\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n3. Install other dependencies:\npip install -r requirements.txt --no-build-isolation\n4. Install FlashAttention:\npip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n5. Prepare the RULER benchmark:\npython scripts/create_data.py \\\n    --data ruler \\\n    --seq_len 4096 8192 16384 32768 65536 \\\n    --model \\\n        meta-llama/Llama-3.2-1B-Instruct \\\n        Qwen/Qwen2.5-0.5B-Instruct",
            "children": []
          },
          {
            "heading": "üß© Example Usage",
            "text": "",
            "children": []
          },
          {
            "heading": "Reproducing Paper Results",
            "text": "Run evaluation (results logged to Weights & Biases):",
            "children": [
              {
                "heading": "SpecKV",
                "text": "python eval.py --cfg cfg/paper/speckv/longbench/llama3_1b_8b/cmax_\n*\n/\n*\n.yaml\npython eval.py --cfg cfg/paper/speckv/longbench/qwen25_05b_14b/cmax_\n*\n/\n*\n.yaml\npython eval.py --cfg cfg/paper/speckv/ruler/\n*\n/llama3_1b_8b/cmax_\n*\n/\n*\n.yaml\npython eval.py --cfg cfg/paper/speckv/ruler/\n*\n/qwen25_05b_14b/cmax_\n*\n/\n*\n.yaml",
                "children": []
              },
              {
                "heading": "SpecPC",
                "text": "python eval.py --cfg cfg/paper/specpc/longbench/llama3_1b_8b/cmax_\n*\n/\n*\n.yaml\npython eval.py --cfg cfg/paper/specpc/longbench/qwen25_05b_14b/cmax_\n*\n/\n*\n.yaml\npython eval.py --cfg cfg/paper/specpc/ruler/\n*\n/llama3_1b_8b/cmax_\n*\n/\n*\n.yaml\npython eval.py --cfg cfg/paper/specpc/ruler/\n*\n/qwen25_05b_14b/cmax_\n*\n/\n*\n.yaml",
                "children": []
              }
            ]
          },
          {
            "heading": "‚è≥ Roadmap",
            "text": "Release codebase for SpecKV and SpecPC\nEnable vLLM compatibility (SpecKV draft, SpecPC target)\nRelease Ada-SpecKV\nRelease Qwen2.5-VL support",
            "children": []
          },
          {
            "heading": "üìñ Citation",
            "text": "If you find this useful, please cite:\n@article\n{\ngalim2025draft\n,\ntitle\n=\n{\nDraft-based Approximate Inference for LLMs\n}\n,\nauthor\n=\n{\nGalim, Kevin and Ewer, Ethan and Kang, Wonjun and Lee, Minjae and Koo, Hyung Il and Lee, Kangwook\n}\n,\njournal\n=\n{\narXiv preprint arXiv:2506.08373\n}\n,\nyear\n=\n{\n2025\n}\n}",
            "children": []
          },
          {
            "heading": "ü§ù Contributions",
            "text": "Pull requests, issues, and feedback are welcome!",
            "children": []
          }
        ]
      }
    ]
  },
  "metadata": {
    "description": null,
    "license": null,
    "stars": 8,
    "forks": 0,
    "topics": [
      "speckv",
      "specpc"
    ],
    "last_updated": "2025-07-19T03:42:14.000Z"
  }
}