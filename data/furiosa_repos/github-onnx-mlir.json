{
  "name": "onnx-mlir",
  "url": "https://github.com/furiosa-ai/onnx-mlir",
  "visibility": "public",
  "readme": {
    "title": "ONNX MLIR",
    "sections": [
      {
        "heading": "ONNX MLIR",
        "text": "The Open Neural Network Exchange implementation in MLIR (\nhttp://onnx.ai/onnx-mlir/\n).\nSystem\nBuild Status\ns390x-Linux\nppc64le-Linux\namd64-Linux\namd64-Windows\namd64-macOS",
        "children": [
          {
            "heading": "Prebuilt Containers",
            "text": "An easy way to get started with ONNX-MLIR is to use a prebuilt docker image.\nThese images are created as a result of a successful merge build on the trunk.\nThis means that the latest image represents the tip of the trunk.\nCurrently there are both Release and Debug mode images for\namd64\n,\nppc64le\nand\ns390x\nsaved in Docker Hub as, respectively,\nonnxmlirczar/onnx-mlir\nand\nonnxmlirczar/onnx-mlir-dev\n.\nTo use one of these images either pull it directly from Docker Hub, launch a container and run an interactive bash shell in it, or use it as the base image in a dockerfile.\nThe onnx-mlir image just contains the built compiler and you can use it immediately to compile your model without any installation. A python convenience script is provided to allow you to run ONNX-MLIR inside a docker container as if running the ONNX-MLIR compiler directly on the host. For example,\n# docker/onnx-mlir.py --EmitLib mnist/model.onnx\n505a5a6fb7d0: Pulling fs layer\n505a5a6fb7d0: Verifying Checksum\n505a5a6fb7d0: Download complete\n505a5a6fb7d0: Pull complete\nShared library model.so has been compiled.\nThe script will pull the onnx-mlir image if it's not available locally, mount the directory containing the\nmodel.onnx\ninto the container, and compile and generate the\nmodel.so\nin the same directory.\nThe onnx-mlir-dev image contains the full build tree including the prerequisites and a clone of the source code.\nThe source can be modified and onnx-mlir rebuilt from within the container, so it is possible to use it\nas a development environment.\nIt is also possible to attach vscode to the running container.\nAn example Dockerfile useful for development and vscode configuration files can be seen in the docs folder.\nIf the workspace directory and the vscode files are not present in the directory where the Docker build is run, then the lines referencing them should be commented out or deleted.\nThe Dockerfile is shown here.\nFROM onnxmlirczar/onnx-mlir-dev\nWORKDIR /workdir\nENV HOME=/workdir\n\n# 1) Install packages.\nENV PATH=$PATH:/workdir/bin\nRUN apt-get update\nRUN apt-get install -y python-numpy\nRUN apt-get install -y python3-pip\nRUN python -m pip install --upgrade pip\nRUN apt-get install -y gdb\nRUN apt-get install -y lldb\nRUN apt-get install -y emacs\nRUN apt-get install -y vim\n# 2) Instal optional packages, uncomment/add as you see fit.\n# RUN apt-get install -y valgrind\n# RUN apt-get install -y libeigen3-dev\n# RUN apt-get install -y clang-format\n# RUN python -m pip install wheel\n# RUN python -m pip install numpy\n# RUN python -m pip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n# RUN git clone https://github.com/onnx/tutorials.git\n# Install clang-12.\n# RUN apt-get install -y lsb-release wget software-properties-common\n# RUN bash -c \"$(wget -O - https://apt.llvm.org/llvm.sh)\"\n\n# 3) When using vscode, copy your .vscode in the Dockerfile dir and\n#    uncomment the two lines below.\n# WORKDIR /workdir/.vscode\n# ADD .vscode /workdir/.vscode\n\n# 4) When using a personal workspace folder, set your workspace sub-directory\n#    in the Dockerfile dir and uncomment the two lines below.\n# WORKDIR /workdir/workspace\n# ADD workspace /workdir/workspace\n\n# 5) Fix git by reattaching head and making git see other branches than master.\nWORKDIR /workdir/onnx-mlir\nRUN git checkout master\nRUN git fetch --unshallow\n\n# 6) Set the PATH environment vars for make/debug mode. Replace Debug\n#    with Release in the PATH below when using Release mode.\nWORKDIR /workdir\nENV MLIR_DIR=/workdir/llvm-project/build/lib/cmake/mlir\nENV NPROC=4\nENV PATH=$PATH:/workdir/onnx-mlir/build/Debug/bin/:/workdir/onnx-mlir/build/Debug/lib:/workdir/llvm-project/build/bin",
            "children": []
          },
          {
            "heading": "Prerequisites",
            "text": "gcc >= 6.4\nlibprotoc >= 3.11.0\ncmake >= 3.15.4\nninja >= 1.10.2\nGCC can be found\nhere\n, or if you have\nHomebrew\n, you can use\nbrew install gcc\n. To check what version of gcc you have installed, run\ngcc --version\n.\nThe instructions to install libprotoc can be found\nhere\n. Or alternatively, if you have Homebrew, you can run\nbrew install protobuf\n. To check what version you have installed, run\nprotoc --version\n.\nCmake can be found\nhere\n. However, to use Cmake, you need to follow the \"How to Install For Command Line Use\" tutorial, which can be found in Cmake under Tools>How to Install For Command Line Use. To check which version you have, you can either look in the desktop version under CMake>About, or run\ncmake --version\n.\nThe instructions for installing Ninja can be found\nhere\n. Or, using Homebrew, you can run\nbrew install ninja\n. To check the version, run\nninja --version\n.\nAt any point in time, ONNX MLIR depends on a specific commit of the LLVM project that has been shown to work with the project. Periodically the maintainers\nneed to move to a more recent LLVM level. Among other things, this requires that the commit string in utils/clone-mlir.sh be updated. A consequence of\nmaking this change is that the TravisCI build will fail until the Docker images that contain the prereqs are rebuilt. There is a GitHub workflow that rebuilds\nthis image for the amd64 architecture, but currently the ppc64le and s390x images must be rebuilt manually. The Dockerfiles to accomplish that are in the repo.",
            "children": []
          },
          {
            "heading": "Installation on UNIX",
            "text": "",
            "children": [
              {
                "heading": "MLIR",
                "text": "Firstly, install MLIR (as a part of LLVM-Project):\ngit clone https://github.com/llvm/llvm-project.git\n#\nCheck out a specific branch that is known to work with ONNX MLIR.\ncd\nllvm-project\n&&\ngit checkout 0bf230d4220660af8b2667506f8905df2f716bdf\n&&\ncd\n..\nmkdir llvm-project/build\ncd\nllvm-project/build\ncmake -G Ninja ../llvm \\\n   -DLLVM_ENABLE_PROJECTS=mlir \\\n   -DLLVM_TARGETS_TO_BUILD=\n\"\nhost\n\"\n\\\n   -DCMAKE_BUILD_TYPE=Release \\\n   -DLLVM_ENABLE_ASSERTIONS=ON \\\n   -DLLVM_ENABLE_RTTI=ON\n\ncmake --build\n.\n--\n${MAKEFLAGS}\ncmake --build\n.\n--target check-mlir",
                "children": []
              },
              {
                "heading": "ONNX-MLIR (this project)",
                "text": "The following environment variables can be set before building onnx-mlir (or alternatively, they need to be passed as CMake variables):\nMLIR_DIR should point to the mlir cmake module inside an llvm-project build or install directory (e.g., llvm-project/build/lib/cmake/mlir).\nThis project uses lit (\nLLVM's Integrated Tester\n) for unit tests. When running CMake, we can also specify the path to the lit tool from LLVM using the LLVM_EXTERNAL_LIT define but it is not required as long as MLIR_DIR points to a build directory of llvm-project. If MLIR_DIR points to an install directory of llvm-project, LLVM_EXTERNAL_LIT is required.\nTo build ONNX-MLIR, use the following commands:\ngit clone --recursive https://github.com/onnx/onnx-mlir.git\n#\nExport environment variables pointing to LLVM-Projects.\nexport\nMLIR_DIR=\n$(\npwd\n)\n/llvm-project/build/lib/cmake/mlir\n\nmkdir onnx-mlir/build\n&&\ncd\nonnx-mlir/build\ncmake -G Ninja ..\ncmake --build\n.\n#\nRun lit tests:\nexport\nLIT_OPTS=-v\ncmake --build\n.\n--target check-onnx-lit\nIf you are running on OSX Big Sur, you need to add\n-DCMAKE_CXX_COMPILER=/usr/bin/c++\nto the\ncmake ..\ncommand due to changes in the compilers.\nAfter the above commands succeed, an\nonnx-mlir\nexecutable should appear in the\nbin\ndirectory.",
                "children": [
                  {
                    "heading": "LLVM and ONNX-MLIR CMake variables",
                    "text": "The following CMake variables from LLVM and ONNX MLIR can be used when compiling ONNX MLIR.\nMLIR_DIR\n:PATH\nPath to to the mlir cmake module inside an llvm-project build or install directory (e.g., c:/repos/llvm-project/build/lib/cmake/mlir).\nThis is required if\nMLIR_DIR\nis not specified as an environment variable.\nLLVM_EXTERNAL_LIT\n:PATH\nPath to the lit tool. Defaults to an empty string and LLVM will find the tool based on\nMLIR_DIR\nif possible.\nThis is required when\nMLIR_DIR\npoints to an install directory.",
                    "children": []
                  }
                ]
              }
            ]
          },
          {
            "heading": "Installation on Windows",
            "text": "Building onnx-mlir on Windows requires building some additional prerequisites that are not available by default.\nNote that the instructions in this file assume you are using\nVisual Studio  2019 Community Edition\nwith ninja. It is recommended that you have the\nDesktop development with C++\nand\nLinux development with C++\nworkloads installed. This ensures you have all toolchains and libraries needed to compile this project and its dependencies on Windows.\nRun all the commands from a shell started from\n\"Developer Command Prompt for VS 2019\"\n.",
            "children": [
              {
                "heading": "Protobuf",
                "text": "Build protobuf as a static library.\ngit clone --recurse-submodules https://github.com/protocolbuffers/protobuf.git\nREM Check out a specific branch that is known to work with ONNX MLIR.\nREM This corresponds to the v3.11.4 tag\ncd\nprotobuf\n&&\ngit checkout d0bfd5221182da1a7cc280f3337b5e41a89539cf\n&&\ncd\n..\nset\nroot_dir=%cd%\nmd protobuf_build\ncd\nprotobuf_build\ncall cmake %root_dir%\n\\p\nrotobuf\n\\c\nmake -G\n\"\nNinja\n\"\n^\n   -DCMAKE_INSTALL_PREFIX=\n\"\n%root_dir%\\protobuf_install\n\"\n^\n   -DCMAKE_BUILD_TYPE=Release ^\n   -Dprotobuf_BUILD_EXAMPLES=OFF ^\n   -Dprotobuf_BUILD_SHARED_LIBS=OFF ^\n   -Dprotobuf_BUILD_TESTS=OFF ^\n   -Dprotobuf_MSVC_STATIC_RUNTIME=OFF ^\n   -Dprotobuf_WITH_ZLIB=OFF\n\ncall cmake --build\n.\n--config Release\ncall cmake --build\n.\n--config Release --target install\nBefore running CMake for onnx-mlir, ensure that the bin directory to this protobuf is before any others in your PATH:\nset\nPATH=%root_dir%\n\\p\nrotobuf_install\n\\b\nin\n;\n%PATH%",
                "children": []
              },
              {
                "heading": "MLIR",
                "text": "Install MLIR (as a part of LLVM-Project):\ngit clone https://github.com/llvm/llvm-project.git\n#\nCheck out a specific branch that is known to work with ONNX MLIR.\ncd\nllvm-project\n&&\ngit checkout 0bf230d4220660af8b2667506f8905df2f716bdf\n&&\ncd\n..\nset\nroot_dir=%cd%\nmd llvm-project\n\\b\nuild\ncd\nllvm-project\n\\b\nuild\ncall cmake %root_dir%\n\\l\nlvm-project\n\\l\nlvm -G\n\"\nNinja\n\"\n^\n   -DCMAKE_INSTALL_PREFIX=\n\"\n%root_dir%\\llvm-project\\build\\install\n\"\n^\n   -DLLVM_ENABLE_PROJECTS=mlir ^\n   -DLLVM_TARGETS_TO_BUILD=\n\"\nhost\n\"\n^\n   -DCMAKE_BUILD_TYPE=Release ^\n   -DLLVM_ENABLE_ASSERTIONS=ON ^\n   -DLLVM_ENABLE_RTTI=ON ^\n   -DLLVM_ENABLE_ZLIB=OFF\n\ncall cmake --build\n.\n--config Release\ncall cmake --build\n.\n--config Release --target install\ncall cmake --build\n.\n--config Release --target check-mlir",
                "children": []
              },
              {
                "heading": "ONNX-MLIR (this project)",
                "text": "The following environment variables can be set before building onnx-mlir (or alternatively, they need to be passed as CMake variables):\nMLIR_DIR should point to the mlir cmake module inside an llvm-project build or install directory (e.g., c:/repos/llvm-project/build/lib/cmake/mlir).\nThis project uses lit (\nLLVM's Integrated Tester\n) for unit tests. When running CMake, we can also specify the path to the lit tool from LLVM using the LLVM_EXTERNAL_LIT define but it is not required as long as MLIR_DIR points to a build directory of llvm-project. If MLIR_DIR points to an install directory of llvm-project, LLVM_EXTERNAL_LIT is required.\nTo build ONNX MLIR, use the following commands:\ngit clone --recursive https://github.com/onnx/onnx-mlir.git\nset\nroot_dir=%cd%\n\nmd onnx-mlir\n\\b\nuild\ncd\nonnx-mlir\n\\b\nuild\ncall cmake %root_dir%\n\\o\nnnx-mlir -G\n\"\nNinja\n\"\n^\n   -DCMAKE_BUILD_TYPE=Release ^\n   -DCMAKE_PREFIX_PATH=%root_dir%\n\\p\nrotobuf_install ^\n   -DLLVM_LIT_ARGS=-v ^\n   -DMLIR_DIR=%root_dir%\n\\l\nlvm-project\n\\b\nuild\n\\l\nib\n\\c\nmake\n\\m\nlir\n\ncall cmake --build\n.\n--config Release --target onnx-mlir\nTo run the lit ONNX MLIR tests, use the following command:\ncall cmake --build\n.\n--config Release --target check-onnx-lit\nTo run the numerical ONNX MLIR tests, use the following command:\ncall cmake --build\n.\n--config Release --target check-onnx-numerical\nTo run the doc ONNX MLIR tests, use the following command after installing third_party ONNX:\ncall cmake --build\n.\n--config Release --target check-docs\nAfter the above commands succeed, an\nonnx-mlir\nexecutable should appear in the\nbin\ndirectory.",
                "children": [
                  {
                    "heading": "LLVM and ONNX-MLIR CMake variables",
                    "text": "The following CMake variables from LLVM and ONNX MLIR can be used when compiling ONNX MLIR.\nMLIR_DIR\n:PATH\nPath to to the mlir cmake module inside an llvm-project build or install directory (e.g., c:/repos/llvm-project/build/lib/cmake/mlir).\nThis is required if\nMLIR_DIR\nis not specified as an environment variable.\nLLVM_EXTERNAL_LIT\n:PATH\nPath to the lit tool. Defaults to an empty string and LLVM will find the tool based on\nMLIR_DIR\nif possible.\nThis is required when\nMLIR_DIR\npoints to an install directory.",
                    "children": []
                  }
                ]
              }
            ]
          },
          {
            "heading": "Using ONNX-MLIR",
            "text": "The usage of\nonnx-mlir\nis as such:\nOVERVIEW: ONNX MLIR modular optimizer driver\n\nUSAGE: onnx-mlir [options] <input file>\n\nOPTIONS:\n\nGeneric Options:\n\n  --help        - Display available options (--help-hidden for more)\n  --help-list   - Display list of available options (--help-list-hidden for more)\n  --version     - Display the version of this program\n\nONNX MLIR Options:\nThese are frontend options.\n\n  Choose target to emit:\n      --EmitONNXBasic - Ingest ONNX and emit the basic ONNX operations without inferred shapes.\n      --EmitONNXIR    - Ingest ONNX and emit corresponding ONNX dialect.\n      --EmitMLIR      - Lower model to MLIR built-in transformation dialect.\n      --EmitLLVMIR    - Lower model to LLVM IR (LLVM dialect).\n      --EmitLib       - Lower model to LLVM IR, emit (to file) LLVM bitcode for model, compile and link it to a shared library.",
            "children": []
          },
          {
            "heading": "Example",
            "text": "For example, to lower an ONNX model (e.g., add.onnx) to ONNX dialect, use the following command:\n./onnx-mlir --EmitONNXIR add.onnx\nThe output should look like:\nmodule\n{\nfunc\n@main_graph\n(\n%arg0:\ntensor\n<\n10\nx\n10\nx\n10\nx\nf32\n>,\n%arg1:\ntensor\n<\n10\nx\n10\nx\n10\nx\nf32\n>) ->\ntensor\n<\n10\nx\n10\nx\n10\nx\nf32\n> {\n%0\n=\n\"\nonnx.Add\n\"\n(\n%arg0\n,\n%arg1\n) : (\ntensor\n<\n10\nx\n10\nx\n10\nx\nf32\n>,\ntensor\n<\n10\nx\n10\nx\n10\nx\nf32\n>) ->\ntensor\n<\n10\nx\n10\nx\n10\nx\nf32\n>\nreturn\n%0\n:\ntensor\n<\n10\nx\n10\nx\n10\nx\nf32\n>\n  }\n}",
            "children": []
          },
          {
            "heading": "Troubleshooting",
            "text": "If the latest LLVM project fails to work due to the latest changes to the MLIR subproject please consider using a slightly older version of LLVM. One such version, which we use, can be found\nhere\n.",
            "children": []
          },
          {
            "heading": "Installing third_party ONNX for Backend Tests or Rebuilding ONNX Operations",
            "text": "Backend tests are triggered by\nmake check-onnx-backend\nin the build directory and require a few preliminary steps to run successfully. Similarily, rebuilding the ONNX operations in ONNX-MLIR from their ONNX descriptions is triggered by\nmake OMONNXOpsIncTranslation\n.\nYou will need to install python 3.x if its not default in your environment, and possibly set the cmake\nPYTHON_EXECUTABLE\nvarialbe in your top cmake file.\nYou will also need\npybind11\nwhich may need to be installed (mac:\nbrew install pybind11\nfor example) and you may need to indicate where to find the software (Mac, POWER, possibly other platforms:\nexport pybind11_DIR=<your path to pybind>\n). Then install the\nthird_party/onnx\nsoftware (Mac:\npip install -e third_party/onnx\n) typed in the top directory.\nOn Macs/POWER and possibly other platforms, there is currently an issue that arises when installing ONNX. If you get an error during the build, try a fix where you edit the top CMakefile as reported in this PR:\nhttps://github.com/onnx/onnx/pull/2482/files\n.",
            "children": []
          },
          {
            "heading": "Slack channel",
            "text": "We have a slack channel established under the Linux Foundation AI and Data Workspace, named\n#onnx-mlir-discussion\n. This channel can be used for asking quick questions related to this project. A direct link is\nhere\n.",
            "children": []
          },
          {
            "heading": "Contributing",
            "text": "Want to contribute, consult this page for specific help on our project\nhere\nor the docs sub-directory.",
            "children": []
          }
        ]
      }
    ]
  },
  "metadata": {
    "description": "Representation and Reference Lowering of ONNX Models in MLIR Compiler Infrastructure",
    "license": null,
    "stars": 0,
    "forks": 0,
    "topics": [],
    "last_updated": "2021-11-17T09:08:25.000Z"
  }
}