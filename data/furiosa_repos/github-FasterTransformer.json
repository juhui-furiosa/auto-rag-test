{
  "name": "FasterTransformer",
  "url": "https://github.com/furiosa-ai/FasterTransformer",
  "visibility": "public",
  "readme": {
    "title": "FasterTransformer",
    "sections": [
      {
        "heading": "FasterTransformer",
        "text": "This repository provides a script and recipe to run the highly optimized transformer-based encoder and decoder component, and it is tested and maintained by NVIDIA.",
        "children": [
          {
            "heading": "Table Of Contents",
            "text": "FasterTransformer\nTable Of Contents\nModel overview\nSupport matrix\nSetup\nRequirements\nQuick Start Guide\nBuild the FasterTransformer\nExecute the encoder demos\nExecute the decoder/decoding demos\nTranslation demos\nGPT demo\nAdvanced\nScripts and sample codes\nCommand-line options\nInference process\nPerformance\nEncoder performance\nEncoder performances of FasterTransformer new features\nEncoder performance on TensorFlow\nEncoder performance on PyTorch\nDecoding and Decoder performance\nDecoder and Decoding end-to-end translation performance on TensorFlow\nDecoder and Decoding end-to-end translation performance on PyTorch\nGPT performance\nRelease notes\nChangelog\nKnown issues\nTODO",
            "children": []
          },
          {
            "heading": "Model overview",
            "text": "In NLP, encoder and decoder are two important components, with the transformer layer becoming a popular architecture for both components. FasterTransformer implements a highly optimized transformer layer for both the encoder and decoder for inference. On Volta, Turing and Ampere GPUs, the computing power of Tensor Cores are used automatically when the precision of the data and weights are FP16.\nFasterTransformer v1.0 provides a highly optimized BERT equivalent Transformer layer for inference, including C++ API, TensorFlow op and TensorRT plugin. The experiments show that FasterTransformer v1 can provide 1.3 ~ 2 times speedup on NVIDIA Tesla T4 and NVIDIA Tesla V100 for inference.\nIn FasterTransformer v2.0, we have added a highly optimized decoder and decoding models based on OpenNMT-TF, an open-source library. Here, the decoder is the model that contains some transformer layers. On the other hand, decoding refers to the whole translating process, including the lookup embedding table, position encoding, a decoder and beam search.\nIn FasterTransformer v2.1, we add some important features. First one is the supporting on PyTorch. Recently, there are more and more PyTorch users. We hope the users of PyTorch can also use the FasterTransformer in their application and research. The second feature is the supporting of\nEffective Transformer\n. This idea is proposed by ByteDance. We call this feature as Effective FasterTransformer It removes the useless padding of encoder input to reduce the computing cost. Third, in addition to decoding with beam search, we also provide the decoding with sampling module. Finally, we optimize many kernels of encoder, decoder and beam search to improve the speed of FasterTransformer.\nIn FasterTransformer v3.0, we implemented the INT8 quantization for encoder (also supporting Effective FasterTransformer). With INT8 quantization, we can take advantage of the powerful INT8 tensor core in Turing GPU to achieve better inference performance (INT8 quantization in FT 3.0 is only supported on device with SM >= 7.5). We also provide quantization tools of tensorflow.\nIn FasterTransformer v3.1, we provide following new features and enhancements. First, we optimize the INT8 kernel of encoder to achieve better performance. Compare to FasterTransformer v3.0, the performance of INT8 quantization brings at most 1.75x speedup. Second, we provide a PyTorch tool to let user be able to train a INT8 quantized model on PyTorch. Besides, FasterTransformer also starts to support the INT8 inference with PyTorch op. So, the users of PyTorch can leverage the INT8 inference. Third, we integrate the fused multi-head attention kernel of TensorRT plugin into FasterTransformer to improve the speed of encoder on Turing and new GPUs. This optimization can bring about 10% ~ 20% speedup compare to original implementation. Finally, we add the supporting of GPT-2 model, which is an important and popular model for decoder.\nIn FasterTransformer v4.0, we provide the multi-nodes multi-gpu inference for GPT model. Compare to usual framework to train giant model like Megatron, FasterTransformer provides 1.2x ~ 3x speedup. Besides, integrating the INT8 fused multi-head attention kernel of TensorRT plugin to further improve the performance of FasterTransformer encoder on INT8. We also add supporting of FP16 fused multi-head attention kernel for V100. Finally, we optimize the decoding module. Compare to v3.1, v4.0 provides at most 2x speedup.\nThe following graph demonstrates the model architecture.\nFig. 1 Encoder-Decoding model architecture.\nFasterTransformer is built on top of CUDA, cuBLAS and cuBLASLt, providing the C++ API and TensorFlow/PyTorch OPs. Users can integrate them into TensorFlow, PyTorch, or other inference service codes that are built in native C++. We also provide some simple sample code to demonstrate how to use the encoder, decoder and to carry out decoding in C++, TensorFlow and PyTorch.\nMore details are in\ndocs/encoder_guide.md\n,\ndocs/decoder_guide.md\nand\ndocs/gpt_guide.md\n. Some common questions and the respective answers are put in\ndocs/QAList.md",
            "children": [
              {
                "heading": "Support matrix",
                "text": "The following matrix shows the architecture differences between the model.\nArchitecure\nEncoder\nEncoder INT8\nquantization\nDecoder\nDecoding with\nbeam search\nDecoding with\nsampling\nGPT-2\nGPT-3\nv1\nYes\nNo\nNo\nNo\nNo\nNo\nNo\nv2\nYes\nNo\nYes\nYes\nNo\nNo\nNo\nv2.1\nYes\nNo\nYes\nYes\nYes\nNo\nNo\nv3.0\nYes\nYes\nYes\nYes\nYes\nNo\nNo\nv3.1\nYes\nYes\nYes\nYes\nYes\nYes\nNo\nv4.0\nYes\nYes\nYes\nYes\nYes\nYes\nYes",
                "children": []
              }
            ]
          },
          {
            "heading": "Setup",
            "text": "The following section lists the requirements to use FasterTransformer.",
            "children": [
              {
                "heading": "Requirements",
                "text": "CMake >= 3.8 for Tensorflow, CMake >= 3.13 for PyTorch\nCUDA 10.1 or newer version\nPython 3 is recommended because some features are not supported in python 2\nTensorflow 1.13 or 1.14 or 1.15\nPyTorch >= 1.5.0\nThese components are readily available within the NGC TensorFlow/PyTorch Docker image below.\nEnsure you have the following components:\nNVIDIA Docker\nand NGC container are recommended\nNVIDIA Pascal\nor\nVolta\nor\nTuring\nor\nAmpere\nbased GPU\nFor more information about how to get started with NGC containers, see the following sections from the NVIDIA GPU Cloud Documentation and the Deep Learning Documentation:\nGetting Started Using NVIDIA GPU Cloud\nAccessing And Pulling From The NGC Container Registry\nRunning TensorFlow\nRunning PyTorch\nFor those unable to use the NGC container, to set up the required environment or create your own container, see the versioned\nNVIDIA Container Support Matrix\n.",
                "children": []
              }
            ]
          },
          {
            "heading": "Quick Start Guide",
            "text": "The following section shows how to use FasterTransformer on the NGC container.",
            "children": [
              {
                "heading": "Build the FasterTransformer",
                "text": "Run the container.\nYou can choose the tensorflow version and python version you want. Here, we list some possible images:\nnvcr.io/nvidia/tensorflow:19.07-py2\ncontains the TensorFlow 1.14 and python 2.7.\nnvcr.io/nvidia/tensorflow:20.12-tf1-py3\ncontains the TensorFlow 1.15 and python 3.8.\nnvcr.io/nvidia/pytorch:20.03-py3\ncontains the PyTorch 1.5.0 and python 3.6\nnvcr.io/nvidia/pytorch:20.07-py3\ncontains the PyTorch 1.6.0 and python 3.6\nnvcr.io/nvidia/pytorch:20.12-py3\ncontains the PyTorch 1.8.0 and python 3.8\nTo achieve best performance, we recommand to use the latest image. For example, running image\nnvcr.io/nvidia/tensorflow:20.12-tf1-py3\nby\nnvidia-docker run -ti --rm nvcr.io/nvidia/tensorflow:20.12-tf1-py3 bash\nClone the repository.\ngit clone https://github.com/NVIDIA/FasterTransformer.git\ncd\nFasterTransformer\nmkdir -p build\ncd\nbuild\nBuild the project.\n3.1 build with C++\ncmake -DSM=xx -DCMAKE_BUILD_TYPE=Release ..\nmake\nNote:\nxx\nis the compute capability of your GPU. For example, 60 (P40) or 61 (P4) or 70 (V100) or 75(T4) or 80 (A100).\n3.2 build with TensorFlow\nUses need to set the path of TensorFlow. For example, if we use\nnvcr.io/nvidia/tensorflow:20.12-tf1-py3\n, then\ncmake -DSM=xx -DCMAKE_BUILD_TYPE=Release -DBUILD_TF=ON -DTF_PATH=/usr/local/lib/python3.8/dist-packages/tensorflow_core/ ..\nmake\nNote:\nxx\nis the compute capability of your GPU. For example, 60 (P40) or 61 (P4) or 70 (V100) or 75(T4) or 80 (A100).\n3.3 build with PyTorch\ncmake -DSM=xx -DCMAKE_BUILD_TYPE=Release -DBUILD_PYT=ON ..\nmake\nNote:\nxx\nis the compute capability of your GPU. For example, 60 (P40) or 61 (P4) or 70 (V100) or 75(T4) or 80 (A100).\nThis will build the TorchScript custom class. Please make sure that the\nPyTorch >= 1.5.0\n.\nNote: From\nFasterTransformer 3.1\n, TorchScript custom op (function type) is deprecated. From\nFasterTransformer 4.0\n, Eager mode PyTorch extension is deprecated.",
                "children": []
              },
              {
                "heading": "Execute the encoder demos",
                "text": "Run FasterTransformer encoder on C++\n./bin/encoder_gemm\n<\nbatch_size\n>\n<\nsequence_length\n>\n<\nhead_number\n>\n<\nsize_per_head\n>\n<\nis_use_fp\n16>\n<\nint8_mode\n>\n./bin/encoder_sample\n<\nbatch_size\n>\n<\nnum_layers\n>\n<\nsequence_length\n>\n<\nhead_number\n>\n<\nsize_per_head\n>\n<\nis_use_fp\n16>\n<\nis_remove_padding\n>\n<\nint8_mode\n>\n<\nallow_gemm_test\n>\n1.1 Run FasterTransformer encoder under FP32 on C++\n./bin/encoder_gemm 32 32 12 64 0 0\n./bin/encoder_sample 32 12 32 12 64 0 0 0 0\n1.2 Run FasterTransformer encoder under FP16 on C++\n./bin/encoder_gemm 32 32 12 64 1 0\n./bin/encoder_sample 32 12 32 12 64 1 0 0 0\n1.3 Run FasterTransformer encoder under INT8 on C++\nWe implement two INT8 pipelines. For int8_mode == 1 (int8v1), we don't quantize residual connection, use int32 as the output of int8 gemms and use per-channel quantization for weights; for int8_mode == 2 (int8v2), we quantize residual connection, use int8 as the output of int8 gemms and use per-tensor quantization for weights. Generally speaking, int8_mode == 1 will have higher accuracy while int8_mode == 2 will have better performance.\nfeature\nint8_mode == 1\nint8_mode == 2\nquantize residual\nNo\nYes\nint8 output gemm\nNo\nYes\nper-channel quantiztion for weights\nYes\nNo\n#\nFor int8_mode == 1\n./bin/encoder_gemm 32 32 12 64 1 1\n./bin/encoder_sample 32 12 32 12 64 1 0 1 0\n#\nFor int8_mode == 2\n./bin/encoder_gemm 32 32 12 64 1 2\n./bin/encoder_sample 32 12 32 12 64 1 0 2 0\n1.4 Run Effective FasterTransformer under FP32 on C++\n./bin/encoder_gemm 32 32 12 64 0 0\n./bin/encoder_sample 32 12 32 12 64 0 1 0 0\n1.5 Run Effective FasterTransformer under INT8 on C++\n#\nFor int8_mode == 1\n./bin/encoder_gemm 32 32 12 64 1 1\n./bin/encoder_sample 32 12 32 12 64 1 1 1 0\n#\nFor int8_mode == 2\n./bin/encoder_gemm 32 32 12 64 1 2\n./bin/encoder_sample 32 12 32 12 64 1 1 2 0\nRun FasterTransformer encoder on TensorFlow\n2.1 Run FasterTransformer encoder under FP32 on TensorFlow\n./bin/encoder_gemm 32 32 12 64 0 0\npython tensorflow/encoder_sample.py \\\n        --batch_size 32 \\\n        --max_seq_len 32 \\\n        --head_number 12 \\\n        --size_per_head 64 \\\n        --num_layer 12 \\\n        --data_type fp32 \\\n        --test_time 1 \\\n        --allow_gemm_test False\nIf use sets\n--test_time 1\n, the program will show the performance of TensorFlow, FasterTransformer and FasterTransformer with removing padding.\n2.2 Run FasterTransformer encoder under FP16 on TensorFlow\n./bin/encoder_gemm 32 32 12 64 1 0\npython tensorflow/encoder_sample.py \\\n        --batch_size 32 \\\n        --max_seq_len 32 \\\n        --head_number 12 \\\n        --size_per_head 64 \\\n        --num_layer 12 \\\n        --data_type fp16 \\\n        --test_time 1 \\\n        --allow_gemm_test False\n2.3 Run FasterTransformer encoder under INT8 on TensorFlow\n#\nFor int8_mode == 1\n./bin/encoder_gemm 32 32 12 64 1 1\npython tensorflow/encoder_sample.py \\\n        --batch_size 32 \\\n        --max_seq_len 32 \\\n        --head_number 12 \\\n        --size_per_head 64 \\\n        --num_layer 12 \\\n        --data_type fp16 \\\n        --test_time 1 \\\n        --int8_mode 1 \\\n        --allow_gemm_test False\n#\nFor int8_mode == 2\n./bin/encoder_gemm 32 32 12 64 1 2\npython tensorflow/encoder_sample.py \\\n        --batch_size 32 \\\n        --max_seq_len 32 \\\n        --head_number 12 \\\n        --size_per_head 64 \\\n        --num_layer 12 \\\n        --data_type fp16 \\\n        --test_time 1 \\\n        --int8_mode 2 \\\n        --allow_gemm_test False\nRun FasterTransformer on PyTorch\nPlease install HuggingFace's\ntransformers\nfirst before run the demos by\npip install transformers==2.5.1\n3.1 Run FasterTransformer encoder under FP32 on PyTorch\n./bin/encoder_gemm 32 32 12 64 0 0\npython pytorch/encoder_sample.py 32 12 32 12 64 --time\n3.2 Run FasterTransformer encoder under FP16 on PyTorch\n./bin/encoder_gemm 32 32 12 64 1 0\npython pytorch/encoder_sample.py 32 12 32 12 64 --fp16 --time\n3.3 Run FasterTransformer encoder under INT8 on PyTorch\n#\nFor int8_mode == 1\n./bin/encoder_gemm 32 32 12 64 1 1\npython pytorch/encoder_sample.py 32 12 32 12 64 --int8_mode 1 --time\n#\nFor int8_mode == 2\n./bin/encoder_gemm 32 32 12 64 1 2\npython pytorch/encoder_sample.py 32 12 32 12 64 --int8_mode 2 --time",
                "children": []
              },
              {
                "heading": "Execute the decoder/decoding demos",
                "text": "Run FasterTransformer decoding on C++\n./bin/decoding_gemm\n<\nbatch_size\n>\n<\nbeam_width\n>\n<\nhead_number\n>\n<\nsize_per_head\n>\n<\nvocab_size\n>\n<\nsequence_length\n>\n<\nencoder_hidden_dim\n>\n<\nis_use_fp\n16>\n./bin/decoding_beamsearch_sample\n<\nbatch_size\n>\n<\nbeam_width\n>\n<\nhead_number\n>\n<\nsize_per_head\n>\n<\nvocab_size\n>\n<\nsequence_length\n>\n<\nnum_layers\n>\n<\nencoder_hidden_dim\n>\n<\nis_use_fp\n16>\n./bin/decoding_sampling_sample\n<\nbatch_size\n>\n<\ncandidate_num\n>\n<\nprobability_threshold\n>\n<\nhead_number\n>\n<\nsize_per_head\n>\n<\nvocab_size\n>\n<\nsequence_length\n>\n<\nnum_layers\n>\n<\nencoder_hidden_dim\n>\n<\nis_use_fp\n16>\n1.1 Run decoding under FP32 on C++\n./bin/decoding_gemm 32 4 8 64 30000 32 512 0\n./bin/decoding_beamsearch_sample 32 4 8 64 30000 32 6 512 0\n#\nbeam search\n./bin/decoding_gemm 32 1 8 64 30000 32 512 0\n./bin/decoding_sampling_sample 32 4 0.0 8 64 30000 32 6 512 0\n#\ntop k sampling\n./bin/decoding_sampling_sample 32 0 0.01 8 64 30000 32 6 512 0\n#\ntop p sampling\n1.2 Run decoding under FP16 on C++\n./bin/decoding_gemm 32 4 8 64 30000 32 512 1\n./bin/decoding_beamsearch_sample 32 4 8 64 30000 32 6 512 1\n#\nbeam search\n./bin/decoding_gemm 32 1 8 64 30000 32 512 1\n./bin/decoding_sampling_sample 32 4 0.0 8 64 30000 32 6 512 1\n#\ntop k sampling\n./bin/decoding_sampling_sample 32 0 0.01 8 64 30000 32 6 512 1\n#\ntop p sampling\nRun FasterTransformer decoder/decoding on TensorFlow\n2.1 Run FasterTransformer decoder under FP32 on TensorFlow\n2.1.1 Verify the correctness\n./bin/decoding_gemm 32 4 8 64 30000 32 512 0\npython tensorflow/decoder_sample.py \\\n        --batch_size 32 \\\n        --beam_width 4 \\\n        --head_number 8 \\\n        --size_per_head 64 \\\n        --vocab_size 30000 \\\n        --max_seq_len 32 \\\n        --num_layer 6 \\\n        --memory_hidden_dim 512 \\\n        --data_type fp32 \\\n        --decoder_type 2\n2.1.2 Test time of TensorFlow decoder\npython tensorflow/decoder_sample.py \\\n        --batch_size 32 \\\n        --beam_width 4 \\\n        --head_number 8 \\\n        --size_per_head 64 \\\n        --vocab_size 30000 \\\n        --max_seq_len 32 \\\n        --num_layer 6 \\\n        --memory_hidden_dim 512 \\\n        --data_type fp32 \\\n        --decoder_type 0 \\\n        --test_time 1\n2.1.3 Test time of FasterTransformer decoder\n./bin/decoding_gemm 32 4 8 64 30000 32 512 0\npython tensorflow/decoder_sample.py \\\n        --batch_size 32 \\\n        --beam_width 4 \\\n        --head_number 8 \\\n        --size_per_head 64 \\\n        --vocab_size 30000 \\\n        --max_seq_len 32 \\\n        --num_layer 6 \\\n        --memory_hidden_dim 512 \\\n        --data_type fp32 \\\n        --decoder_type 1 \\\n        --test_time 1\n2.2 Run FasterTransformer decoder under FP16 on TensorFlow\n./bin/decoding_gemm 32 4 8 64 30000 32 512 1\npython tensorflow/decoder_sample.py \\\n        --batch_size 32 \\\n        --beam_width 4 \\\n        --head_number 8 \\\n        --size_per_head 64 \\\n        --vocab_size 30000 \\\n        --max_seq_len 32 \\\n        --num_layer 6 \\\n        --memory_hidden_dim 512 \\\n        --data_type fp16 \\\n        --decoder_type 2\n2.3 Run FasterTransformer decoding under FP32 on TensorFlow\n./bin/decoding_gemm 32 4 8 64 30000 32 512 0\npython tensorflow/decoding_sample.py \\\n        --batch_size 32 \\\n        --beam_width 4 \\\n        --head_number 8 \\\n        --size_per_head 64 \\\n        --vocab_size 30000 \\\n        --max_seq_len 32 \\\n        --num_layer 6 \\\n        --memory_hidden_dim 512 \\\n        --data_type fp32 \\\n        --beam_search_diversity_rate -1.3 \\\n        --sampling_topk 0 \\\n        --sampling_topp 0.01 \\\n        --test_time 0123\n2.4 Run FasterTransformer decoding under FP16 on TensorFlow\n./bin/decoding_gemm 32 4 8 64 30000 32 512 1\npython tensorflow/decoding_sample.py \\\n        --batch_size 32 \\\n        --beam_width 4 \\\n        --head_number 8 \\\n        --size_per_head 64 \\\n        --vocab_size 30000 \\\n        --max_seq_len 32 \\\n        --num_layer 6 \\\n        --memory_hidden_dim 512 \\\n        --data_type fp16 \\\n        --beam_search_diversity_rate -1.3 \\\n        --sampling_topk 0 \\\n        --sampling_topp 0.01 \\\n        --test_time 0123\nRun FasterTransformer decoder/decoding on PyTorch\nPlease install OpenNMT-py first before running the demos by\npip install opennmt-py==1.1.1\n3.1 Run FasterTransformer decoder under FP32 on PyTorch\n./bin/decoding_gemm 8 4 8 64 31538 32 512 0\npython pytorch/decoder_sample.py 8 6 32 8 64 --time\n3.2 Run FasterTransformer decoder under FP16 on PyTorch\n./bin/decoding_gemm 8 4 8 64 31538 32 512 1\npython pytorch/decoder_sample.py 8 6 32 8 64 --fp16 --time\n3.3 Run FasterTransformer decoding under FP32 on PyTorch\n./bin/decoding_gemm 8 4 8 64 31538 32 512 0\npython pytorch/decoding_sample.py 8 6 32 8 64 4 31538 --time\n3.4 Run FasterTransformer decoding under FP16 on PyTorch\n./bin/decoding_gemm 8 4 8 64 31538 32 512 1\npython pytorch/decoding_sample.py 8 6 32 8 64 4 31538 --fp16 --time",
                "children": []
              },
              {
                "heading": "Translation demos",
                "text": "Translation with FasterTransformer on TensorFlow\n1.1 Prepare data and model\nbash tensorflow/utils/translation/download_model_data.sh\n1.2 Run under FP32\n./bin/decoding_gemm 128 4 8 64 32001 100 512 0\npython tensorflow/translate_sample.py \\\n        --batch_size 128 \\\n        --beam_width 4 \\\n        --encoder_head_number 8 \\\n        --encoder_size_per_head 64 \\\n        --decoder_head_number 8 \\\n        --decoder_size_per_head 64 \\\n        --max_seq_len 32 \\\n        --encoder_num_layer 6 \\\n        --decoder_num_layer 6 \\\n        --data_type fp32 \\\n        --beam_search_diversity_rate 0.0 \\\n        --sampling_topk 1 \\\n        --sampling_topp 0.00 \\\n        --test_time 012345\n1.3 Run under FP16\npython tensorflow/tensorflow_bert/ckpt_type_convert.py --init_checkpoint=translation/ckpt/model.ckpt-500000 --fp16_checkpoint=translation/ckpt/fp16_model.ckpt-500000\n./bin/decoding_gemm 128 4 8 64 32001 100 512 1\npython tensorflow/translate_sample.py \\\n      --batch_size 128 \\\n      --beam_width 4 \\\n      --encoder_head_number 8 \\\n      --encoder_size_per_head 64 \\\n      --decoder_head_number 8 \\\n      --decoder_size_per_head 64 \\\n      --max_seq_len 32 \\\n      --encoder_num_layer 6 \\\n      --decoder_num_layer 6 \\\n      --data_type fp16 \\\n      --beam_search_diversity_rate 0.0 \\\n      --sampling_topk 1 \\\n      --sampling_topp 0.00 \\\n      --test_time 012345\nTranslation with FasterTransformer on PyTorch\n2.1 Prepare model and data\nbash pytorch/scripts/download_translation_model.sh\n2.2 Run under FP32\n./bin/decoding_gemm 128 4 8 64 31538 100 512 0\npython pytorch/run_translation.py --batch_size 128 --beam_size 4 --model_type decoding_ext --data_type fp32\n2.3 Run under FP16\n./bin/decoding_gemm 128 4 8 64 31538 100 512 1\npython pytorch/run_translation.py --batch_size 128 --beam_size 4 --model_type decoding_ext --data_type fp16",
                "children": []
              },
              {
                "heading": "GPT demo",
                "text": "Here, we demonstrate how to run Fastertransformer on Megatron model with C++ and PyTorch api. More details are in\ndocs/gpt_guide.md\n.\nPrepare\npip install -r ../requirement.txt\nwget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json -P models\nwget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -P models\nwget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O megatron_lm_345m_v0.0.zip\nmkdir -p models/megatron-models/345m\nunzip megatron_lm_345m_v0.0.zip -d models/megatron-models/345m\ngit clone https://github.com/NVIDIA/Megatron-LM.git\npython ../sample/pytorch/utils/megatron_ckpt_convert.py -i ./models/megatron-models/345m/release/ -o ./models/megatron-models/c-model/345m/ -t_g 1 -i_g 1\nNote that there are different checkpoint version of Megatron. The version of the checkpoint above is 0. If users have trained a model by themselves, the default version of latest Megatron is 3. To convert the checkpoint with version 3, please add\n-checkpoint_version 3\n.\nRun GPT\n2.1 Run on C++\nUsers can see the details of arguments in\nsample/cpp/gpt_config.ini\n. It controls the model path, model size, tensor parallelism size, and some hyper-parameters. And then run gpt by following script:\n./bin/gpt_sample\npython ../sample/pytorch/utils/convert_gpt_token.py --vocab_file=./models/gpt2-vocab.json  --bpe_file=./models/gpt2-merges.txt\nThe following script run multi-gpus (Note that users need to modify the\ngpt_config.ini\n. For example, set\ntensor_para_size\nto 8.)\nmpirun -n 8 ./bin/gpt_sample\npython ../sample/pytorch/utils/convert_gpt_token.py --vocab_file=./models/gpt2-vocab.json  --bpe_file=./models/gpt2-merges.txt\n2.2 Run on Pytorch\n#\nNo parallelism (tensor_para_size=1, layer_para_size=1)\nmpirun -n 1 --allow-run-as-root python ./pytorch/gpt_sample.py\n#\nTP (tensor_para_size=8, layer_para_size=1)\nmpirun -n 8 --allow-run-as-root python ./pytorch/gpt_sample.py --tensor_para_size=8 --layer_para_size=1 --ckpt_path=\n\"\n/workspace/fastertransformer/models/megatron-models/c-model/345m/8-gpu\n\"",
                "children": []
              }
            ]
          },
          {
            "heading": "Advanced",
            "text": "The following sections provide greater details.",
            "children": [
              {
                "heading": "Scripts and sample codes",
                "text": "The following code lists the directory structure of FasterTransformer:\n/fastertransformer:\nsource\ncode of transformer\n|\n--/cuda: some CUDA kernels and multi-head attention implementation, both are compiled with cuda/cuBLAS/cuBLASLt.\n|\n--/tf_op: custom Tensorflow OP implementation\n|\n--/th_op: custom PyTorch OP implementation\n|\n--/triton_backend: custom triton backend implementation\n|\n--/trt_fused_multihead_attention: fused multihead attention kernels of TensorRT\n/sample: C++ and tensorflow transformer interface samples\n|\n--/cpp: C++ interface samples\n|\n--/pytorch: PyTorch OP samples\n|\n--/tensorflow: TensorFlow OP samples\n|\n--/tensorflow_bert: samples that show of how to integrate our Tensorflow OP into the open\nsource\nBERT model\nfor\nsentence (and sentence-pair) classification tasks (GLUE), the samples support both FP16 and FP32, see readme file within this folder more details\n/tools/gemm_test: loop over all GEMM algorithms to pick the best one\n/bert-quantization/\n|\n--bert-tf-quantization: TensorFlow quantization tool and sample codes\n|\n--bert-pyt-quantization/: PyTorch quantization sample codes\n/docs/\nIn the root directory of FasterTransformer, the most important directories are:\nfastertransformer/\nsample/\ntools/\nbert-quantization/\ndocs/\nThe\nfastertransformer/\nfolder encapsulates all the source codes of FasterTransformer:\ntf_op/\n- Contains the TensorFlow Op source files of encoder, decoder and decoding\nth_op/\n- Contains the PyTorch Op source files of encoder, decoder and decoding\ncuda/\n- Contains all CUDA kernels of FasterTransformer\nbert_encoder_transformer.h\n- Contains the encoder transformer layer\nopen_decoder.h\n- Contains the decoder transformer layer\ndecoding_beamsearch.h\n- Contains the progress of decoding with beam search\ndecoding_sampling.h\n- Contains the progress of decoding with beam search\ngpt.h\n- Contains the progress of GPT\nThe\ntools/\nfolder contains the tools to generate the GEMM configuration of FasterTransformer for different settings:\ntools/gemm_test/encoder_gemm.cc\n- Encoder GEMM config\ntools/gemm_test/decoding_gemm.cc\n- Decoder and decoding GEMM config\nThe\nsample/\nfolder contains useful sample codes for FasterTransformer:\nsample/cpp/encoder_sample.cc\n- C encoder sample codes\nsample/cpp/decoding_beamsearch_sample.cc\n- C decoding with beam search sample codes\nsample/cpp/decoding_sampling_sample.cc\n- C decoding with sampling sample codes\nsample/cpp/gpt_sample.cc\n- C GPT codes\nsample/tensorflow/encoder_sample.py\n- TensorFlow encoder sample codes\nsample/tensorflow/decoder_sample.py\n- TensorFlow decoder sample codes\nsample/tensorflow/decoding_sample.py\n- TensorFlow decoding sample codes\nsample/tensorflow/tensorflow_bert/\n- TensorFlow using FasterTransformer in BERT sample codes\nsample/tensorflow/translate_sample.py\n- TensorFlow translation sample codes\nsample/tensorflow/gpt_sample.py\n- TensorFlow GPT sample codes\nsample/pytorch/encoder_sample.py\n- PyTorch encoder sample codes\nsample/pytorch/decoder_sample.py\n- PyTorch decoder sample codes\nsample/pytorch/decoding_sample.py\n- PyTorch decoding sample codes\nsample/pytorch/run_glue.py\n- PyTorch BERT on GLUE dataset sample codes\nsample/pytorch/run_squad.py\n- PyTorch BERT on SQuAD dataset sample codes\nsample/pytorch/run_translation.py\n- PyTorch decoding for translation sample codes",
                "children": []
              },
              {
                "heading": "Command-line options",
                "text": "To see the full list of available options and their descriptions, use the\n-h\nor\n--help\ncommand-line option with the Python file, for example:\npython tensorflow/encoder_sample.py --help\npython tensorflow/decoder_sample.py --help\npython tensorflow/decoding_sample.py --help\npython tensorflow/translate_sample.py --help",
                "children": []
              },
              {
                "heading": "Inference process",
                "text": "This subsection provides the details about how to use the encoder, the decoder and the decoding.",
                "children": []
              }
            ]
          },
          {
            "heading": "Performance",
            "text": "Hardware settings:\n8xA100-80GBs (with mclk 1593MHz, pclk 1410MHz) with AMD EPYC 7742 64-Core Processor\nT4 (with mclk 5000MHz, pclk 1590MHz) with Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz\nIn order to run the following benchmark, we need to install the unix computing tool \"bc\" by\napt-get install bc",
            "children": [
              {
                "heading": "Encoder performance",
                "text": "The FP16 results of TensorFlow were obtained by running the\nsample/tensorflow/scripts/profile_encoder_performance.sh\n.\nThe INT8 results of TensorFlow were obtained by running the\nsample/tensorflow/scripts/profile_encoder_performance_int8.sh\n.\nThe FP16 results of PyTorch were obtained by running the\nsample/pytorch/scripts/profile_encoder.sh\n.\nThe INT8 results of PyTorch were obtained by running the\nsample/pytorch/scripts/profile_encoder_int8.sh\n.\nIn the experiments of encoder, we updated the following parameters:\nhead_num = 12\nsize_per_head = 64\nnum_layers = 12\nMore benchmarks are put in\ndocs/encoder_guide.md\n.",
                "children": [
                  {
                    "heading": "Encoder performances of FasterTransformer new features",
                    "text": "The following figure compares the performances of different features of FasterTransformer and FasterTransformer under FP16 on T4.\nFor large batch size and sequence length, both EFF-FT and FT-INT8-v2 bring about 2x speedup. Using Effective FasterTransformer and int8v2 at the same time can bring about 3.5x speedup compared to FasterTransformer FP16 for large case.",
                    "children": []
                  },
                  {
                    "heading": "Encoder performance on TensorFlow",
                    "text": "The following figure compares the performances of different features of FasterTransformer and TensorFlow XLA under FP16 on T4.\nFor small batch size and sequence length, using FasterTransformer can bring about 3x speedup.\nFor large batch size and sequence length, using Effective FasterTransformer with INT8-v2 quantization can bring about 5x speedup.",
                    "children": []
                  },
                  {
                    "heading": "Encoder performance on PyTorch",
                    "text": "The following figure compares the performances of different features of FasterTransformer and PyTorch TorchScript under FP16 on T4.\nFor small batch size and sequence length, using FasterTransformer CustomExt can bring about 4x ~ 6x speedup.\nFor large batch size and sequence length, using Effective FasterTransformer with INT8-v2 quantization can bring about 5x speedup.",
                    "children": []
                  }
                ]
              },
              {
                "heading": "Decoding and Decoder performance",
                "text": "The results of TensorFlow were obtained by running the\nprofile_decoding_beamsearch_performance.sh\nand\nprofile_decoding_sampling_performance.sh\nThe results of PyTorch were obtained by running the\nprofile_decoder_decoding.sh\n.\nIn the experiments of decoding, we updated the following parameters:\nhead_num = 8\nsize_per_head = 64\nnum_layers = 6 for both encoder and decoder\nvocabulary_size = 30000 for TensorFlow sample codes, 31538 for PyTorch sample codes\nmemory_hidden_dim = 512\nmax sequenc elength = 128\nMore benchmarks are put in\ndocs/decoder_guide.md\n.",
                "children": [
                  {
                    "heading": "Decoder and Decoding end-to-end translation performance on TensorFlow",
                    "text": "The following figure shows the speedup of of FT-Decoder op and FT-Decoding op compared to TensorFlow under FP16 with T4. Here, we use the throughput of translating a test set to prevent the total tokens of each methods may be different. Compared to TensorFlow, FT-Decoder provides 1.5x ~ 3x speedup; while FT-Decoding provides 4x ~ 18x speedup.",
                    "children": []
                  },
                  {
                    "heading": "Decoder and Decoding end-to-end translation performance on PyTorch",
                    "text": "The following figure shows the speedup of of FT-Decoder op and FT-Decoding op compared to PyTorch under FP16 with T4. Here, we use the throughput of translating a test set to prevent the total tokens of each methods may be different. Compared to PyTorch, FT-Decoder provides 1.2x ~ 3x speedup; while FT-Decoding provides 3.8x ~ 13x speedup.",
                    "children": []
                  }
                ]
              },
              {
                "heading": "GPT performance",
                "text": "The following figure compares the performances of Megatron and FasterTransformer under FP16 on A100.\nIn the experiments of decoding, we updated the following parameters:\nhead_num = 96\nsize_per_head = 128\nnum_layers = 48 for GPT-89B model, 96 for GPT-175B model\ndata_type = FP16\nvocab_size = 51200\ntop_p = 0.9\ntensor parallel size = 8\ninput sequence length = 512\nouptut sequence length = 32",
                "children": []
              }
            ]
          },
          {
            "heading": "Release notes",
            "text": "",
            "children": [
              {
                "heading": "Changelog",
                "text": "June 2021\nSupport XLNet\nApril 2021\nSupport multi-gpus and multi-nodes inference for GPT model on C++ and PyTorch.\nSupport single node, multi-gpus inference for GPT model on triton.\nAdd the int8 fused multi-head attention kernel for bert.\nAdd the FP16 fused multi-head attention kernel of V100 for bert.\nOptimize the kernel of decoder.\nMove to independent repo.\nRelease the FasterTransformer 4.0\nDec 2020\nOptimize the decoding by adding the finisehd mask to prevent useless computing.\nSupport opennmt encoder.\nRemove the TensorRT plugin supporting.\nRelease the FasterTransformer 3.1\nNov 2020\nOptimize the INT8 inference.\nSupport PyTorch INT8 inference.\nProvide PyTorch INT8 quantiztion tools.\nIntegrate the fused multi-head attention kernel of TensorRT into FasterTransformer.\nAdd unit test of SQuAD.\nUpdate the missed NGC checkpoints.\nSep 2020\nSupport GPT2\nRelease the FasterTransformer 3.0\nSupport INT8 quantization of encoder of cpp and TensorFlow op.\nAdd bert-tf-quantization tool.\nFix the issue that Cmake 15 or Cmake 16 fail to build this project.\nAug 2020\nFix the bug of trt plugin.\nJune 2020\nRelease the FasterTransformer 2.1\nAdd Effective FasterTransformer based on the idea of\nEffective Transformer\nidea.\nOptimize the beam search kernels.\nAdd PyTorch op supporting\nMay 2020\nFix the bug that seq_len of encoder must be larger than 3.\nAdd the position_encoding of decoding as the input of FasterTransformer decoding. This is convenient to use different types of position encoding. FasterTransformer does not compute the position encoding value, but only lookup the table.\nModifying the method of loading model in\ntranslate_sample.py\n.\nApril 2020\nRename\ndecoding_opennmt.h\nto\ndecoding_beamsearch.h\nAdd DiverseSiblingsSearch for decoding.\nAdd sampling into Decoding\nThe implementation is in the\ndecoding_sampling.h\nAdd top_k sampling, top_p sampling for decoding.\nRefactor the tensorflow custom op codes.\nMerge\nbert_transformer_op.h\n,\nbert_transformer_op.cu.cc\ninto\nbert_transformer_op.cc\nMerge\ndecoder.h\n,\ndecoder.cu.cc\ninto\ndecoder.cc\nMerge\ndecoding_beamsearch.h\n,\ndecoding_beamsearch.cu.cc\ninto\ndecoding_beamsearch.cc\nFix the bugs of finalize function decoding.py.\nFix the bug of tf DiverseSiblingSearch.\nAdd BLEU scorer\nbleu_score.py\ninto\nutils\n. Note that the BLEU score requires python3.\nFuse QKV Gemm of encoder and masked_multi_head_attention of decoder.\nAdd dynamic batch size and dynamic sequence length features into all ops.\nMarch 2020\nAdd feature in FasterTransformer 2.0\nAdd\ntranslate_sample.py\nto demonstrate how to translate a sentence by restoring the pretrained model of OpenNMT-tf.\nFix bugs of Fastertransformer 2.0\nFix the bug of maximum sequence length of decoder cannot be larger than 128.\nFix the bug that decoding does not check finish or not after each step.\nFix the bug of decoder about max_seq_len.\nModify the decoding model structure to fit the OpenNMT-tf decoding model.\nAdd a layer normalization layer after decoder.\nAdd a normalization for inputs of decoder\nFebuary 2020\nRelease the FasterTransformer 2.0\nProvide a highly optimized OpenNMT-tf based decoder and decoding, including C++ API and TensorFlow op.\nRefine the sample codes of encoder.\nAdd dynamic batch size feature into encoder op.\nJuly 2019\nRelease the FasterTransformer 1.0\nProvide a highly optimized bert equivalent transformer layer, including C++ API, TensorFlow op and TensorRT plugin.",
                "children": []
              },
              {
                "heading": "Known issues",
                "text": "Undefined symbol errors when import the extension\nPlease\nimport torch\nfirst. If this has been done, it is due to the incompatible C++ ABI. You may need to check the PyTorch used during compilation and execution are the same, or you need to check how your PyTorch is compiled, or the version of your GCC, etc.\nResults of TensorFlow and OP would be different in decoding. This problem is caused by the accumulated log probability, and we do not avoid this problem.\nIf encounter some problem in the custom environment, try to use the gcc/g++ 4.8 to build the project of TensorFlow op, especially for TensorFlow 1.14.",
                "children": []
              },
              {
                "heading": "TODO",
                "text": "Support the decoding sampling in PyTorch.",
                "children": []
              }
            ]
          }
        ]
      }
    ]
  },
  "metadata": {
    "description": "Transformer related optimization, including BERT, GPT",
    "license": null,
    "stars": 0,
    "forks": 0,
    "topics": [],
    "last_updated": "2021-11-30T10:32:49.000Z"
  }
}