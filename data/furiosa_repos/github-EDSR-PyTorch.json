{
  "name": "EDSR-PyTorch",
  "url": "https://github.com/furiosa-ai/EDSR-PyTorch",
  "visibility": "public",
  "readme": {
    "title": "EDSR-PyTorch",
    "sections": [
      {
        "heading": "EDSR-PyTorch",
        "text": "About PyTorch 1.1.0\nThere have been minor changes with the 1.1.0 update. Now we support PyTorch 1.1.0 by default, and please use the legacy branch if you prefer older version.\nThis repository is an official PyTorch implementation of the paper\n\"Enhanced Deep Residual Networks for Single Image Super-Resolution\"\nfrom\nCVPRW 2017, 2nd NTIRE\n.\nYou can find the original code and more information from\nhere\n.\nIf you find our work useful in your research or publication, please cite our work:\n[1] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee,\n\"Enhanced Deep Residual Networks for Single Image Super-Resolution,\"\n2nd NTIRE: New Trends in Image Restoration and Enhancement workshop and challenge on image super-resolution in conjunction with\nCVPR 2017\n.\n[\nPDF\n] [\narXiv\n] [\nSlide\n]\n@InProceedings{Lim_2017_CVPR_Workshops,\n  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},\n  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n  month = {July},\n  year = {2017}\n}\nWe provide scripts for reproducing all the results from our paper. You can train your model from scratch, or use a pre-trained model to enlarge your images.\nDifferences between Torch version\nCodes are much more compact. (Removed all unnecessary parts.)\nModels are smaller. (About half.)\nSlightly better performances.\nTraining and evaluation requires less memory.\nPython-based.",
        "children": [
          {
            "heading": "Dependencies",
            "text": "Python 3.6\nPyTorch >= 1.0.0\nnumpy\nskimage\nimageio\nmatplotlib\ntqdm\ncv2 >= 3.xx (Only if you want to use video input/output)",
            "children": []
          },
          {
            "heading": "Code",
            "text": "Clone this repository into any place you want.\ngit clone https://github.com/thstkdgus35/EDSR-PyTorch\ncd\nEDSR-PyTorch",
            "children": []
          },
          {
            "heading": "Quickstart (Demo)",
            "text": "You can test our super-resolution algorithm with your images. Place your images in\ntest\nfolder. (like\ntest/<your_image>\n) We support\npng\nand\njpeg\nfiles.\nRun the script in\nsrc\nfolder. Before you run the demo, please uncomment the appropriate line in\ndemo.sh\nthat you want to execute.\ncd\nsrc\n#\nYou are now in */EDSR-PyTorch/src\nsh demo.sh\nYou can find the result images from\nexperiment/test/results\nfolder.\nModel\nScale\nFile name (.pt)\nParameters\n**\nPSNR\nEDSR\n2\nEDSR_baseline_x2\n1.37 M\n34.61 dB\n*EDSR_x2\n40.7 M\n35.03 dB\n3\nEDSR_baseline_x3\n1.55 M\n30.92 dB\n*EDSR_x3\n43.7 M\n31.26 dB\n4\nEDSR_baseline_x4\n1.52 M\n28.95 dB\n*EDSR_x4\n43.1 M\n29.25 dB\nMDSR\n2\nMDSR_baseline\n3.23 M\n34.63 dB\n*MDSR\n7.95 M\n34.92 dB\n3\nMDSR_baseline\n30.94 dB\n*MDSR\n31.22 dB\n4\nMDSR_baseline\n28.97 dB\n*MDSR\n29.24 dB\n*Baseline models are in\nexperiment/model\n. Please download our final models from\nhere\n(542MB)\n**We measured PSNR using DIV2K 0801 ~ 0900, RGB channels, without self-ensemble. (scale + 2) pixels from the image boundary are ignored.\nYou can evaluate your models with widely-used benchmark datasets:\nSet5 - Bevilacqua et al. BMVC 2012\n,\nSet14 - Zeyde et al. LNCS 2010\n,\nB100 - Martin et al. ICCV 2001\n,\nUrban100 - Huang et al. CVPR 2015\n.\nFor these datasets, we first convert the result images to YCbCr color space and evaluate PSNR on the Y channel only. You can download\nbenchmark datasets\n(250MB). Set\n--dir_data <where_benchmark_folder_located>\nto evaluate the EDSR and MDSR with the benchmarks.\nYou can download some results from\nhere\n.\nThe link contains\nEDSR+_baseline_x4\nand\nEDSR+_x4\n.\nOtherwise, you can easily generate result images with\ndemo.sh\nscripts.",
            "children": []
          },
          {
            "heading": "How to train EDSR and MDSR",
            "text": "We used\nDIV2K\ndataset to train our model. Please download it from\nhere\n(7.1GB).\nUnpack the tar file to any place you want. Then, change the\ndir_data\nargument in\nsrc/option.py\nto the place where DIV2K images are located.\nWe recommend you to pre-process the images before training. This step will decode all\npng\nfiles and save them as binaries. Use\n--ext sep_reset\nargument on your first run. You can skip the decoding part and use saved binaries with\n--ext sep\nargument.\nIf you have enough RAM (>= 32GB), you can use\n--ext bin\nargument to pack all DIV2K images in one binary file.\nYou can train EDSR and MDSR by yourself. All scripts are provided in the\nsrc/demo.sh\n. Note that EDSR (x3, x4) requires pre-trained EDSR (x2). You can ignore this constraint by removing\n--pre_train <x2 model>\nargument.\ncd\nsrc\n#\nYou are now in */EDSR-PyTorch/src\nsh demo.sh\nUpdate log\nJan 04, 2018\nMany parts are re-written. You cannot use previous scripts and models directly.\nPre-trained MDSR is temporarily disabled.\nTraining details are included.\nJan 09, 2018\nMissing files are included (\nsrc/data/MyImage.py\n).\nSome links are fixed.\nJan 16, 2018\nMemory efficient forward function is implemented.\nAdd --chop_forward argument to your script to enable it.\nBasically, this function first split a large image to small patches. Those images are merged after super-resolution. I checked this function with 12GB memory, 4000 x 2000 input image in scale 4. (Therefore, the output will be 16000 x 8000.)\nFeb 21, 2018\nFixed the problem when loading pre-trained multi-GPU model.\nAdded pre-trained scale 2 baseline model.\nThis code now only saves the best-performing model by default. For MDSR, 'the best' can be ambiguous. Use --save_models argument to keep all the intermediate models.\nPyTorch 0.3.1 changed their implementation of DataLoader function. Therefore, I also changed my implementation of MSDataLoader. You can find it on feature/dataloader branch.\nFeb 23, 2018\nNow PyTorch 0.3.1 is a default. Use legacy/0.3.0 branch if you use the old version.\nWith a new\nsrc/data/DIV2K.py\ncode, one can easily create new data class for super-resolution.\nNew binary data pack. (Please remove the\nDIV2K_decoded\nfolder from your dataset if you have.)\nWith\n--ext bin\n, this code will automatically generate and saves the binary data pack that corresponds to previous\nDIV2K_decoded\n. (This requires huge RAM (~45GB, Swap can be used.), so please be careful.)\nIf you cannot make the binary pack, use the default setting (\n--ext img\n).\nFixed a bug that PSNR in the log and PSNR calculated from the saved images does not match.\nNow saved images have better quality! (PSNR is ~0.1dB higher than the original code.)\nAdded performance comparison between Torch7 model and PyTorch models.\nMar 5, 2018\nAll baseline models are uploaded.\nNow supports half-precision at test time. Use\n--precision half\nto enable it. This does not degrade the output images.\nMar 11, 2018\nFixed some typos in the code and script.\nNow --ext img is default setting. Although we recommend you to use --ext bin when training, please use --ext img when you use --test_only.\nSkip_batch operation is implemented. Use --skip_threshold argument to skip the batch that you want to ignore. Although this function is not exactly the same with that of Torch7 version, it will work as you expected.\nMar 20, 2018\nUse\n--ext sep-reset\nto pre-decode large png files. Those decoded files will be saved to the same directory with DIV2K png files. After the first run, you can use\n--ext sep\nto save time.\nNow supports various benchmark datasets. For example, try\n--data_test Set5\nto test your model on the Set5 images.\nChanged the behavior of skip_batch.\nMar 29, 2018\nWe now provide all models from our paper.\nWe also provide\nMDSR_baseline_jpeg\nmodel that suppresses JPEG artifacts in the original low-resolution image. Please use it if you have any trouble.\nMyImage\ndataset is changed to\nDemo\ndataset. Also, it works more efficient than before.\nSome codes and script are re-written.\nApr 9, 2018\nVGG and Adversarial loss is implemented based on\nSRGAN\n.\nWGAN\nand\ngradient penalty\nare also implemented, but they are not tested yet.\nMany codes are refactored. If there exists a bug, please report it.\nD-DBPN\nis implemented. The default setting is D-DBPN-L.\nApr 26, 2018\nCompatible with PyTorch 0.4.0\nPlease use the legacy/0.3.1 branch if you are using the old version of PyTorch.\nMinor bug fixes\nJuly 22, 2018\nThanks for recent commits that contains RDN and RCAN. Please see\ncode/demo.sh\nto train/test those models.\nNow the dataloader is much stable than the previous version. Please erase\nDIV2K/bin\nfolder that is created before this commit. Also, please avoid using\n--ext bin\nargument. Our code will automatically pre-decode png images before training. If you do not have enough spaces(~10GB) in your disk, we recommend\n--ext img\n(But SLOW!).\nOct 18, 2018\nwith\n--pre_train download\n, pretrained models will be automatically downloaded from the server.\nSupports video input/output (inference only). Try with\n--data_test video --dir_demo [video file directory]\n.\nAbout PyTorch 1.0.0\nWe support PyTorch 1.0.0. If you prefer the previous versions of PyTorch, use legacy branches.\n--ext bin\nis not supported. Also, please erase your bin files with\n--ext sep-reset\n. Once you successfully build those bin files, you can remove\n-reset\nfrom the argument.",
            "children": []
          }
        ]
      }
    ]
  },
  "metadata": {
    "description": "PyTorch version of the paper 'Enhanced Deep Residual Networks for Single Image Super-Resolution' (CVPRW 2017)",
    "license": null,
    "stars": 0,
    "forks": 0,
    "topics": [],
    "last_updated": "2022-03-22T09:12:57.000Z"
  }
}