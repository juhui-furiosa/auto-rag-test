{
  "name": "furiosa-dra-driver-guide",
  "url": "https://github.com/furiosa-ai/furiosa-dra-driver-guide",
  "visibility": "public",
  "readme": {
    "title": "Furiosa DRA Driver Guide",
    "sections": [
      {
        "heading": "Furiosa DRA Driver Guide",
        "text": "This guide provides instructions for enabling Dynamic Resource Allocation (DRA) in Kubernetes v1.33, installing Furiosa DRA driver and requesting Furiosa NPU through DRA API.",
        "children": [
          {
            "heading": "Installing v1.33 Kubernetes Binaries",
            "text": "Run the following commands to install Kubernetes v1.33 binaries on Ubuntu\nSee\nInstalling Kubeadm\nfor more details.\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key\n|\nsudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho\n'\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /\n'\n|\nsudo tee /etc/apt/sources.list.d/kubernetes.list\n\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\nsudo systemctl\nenable\n--now kubelet",
            "children": []
          },
          {
            "heading": "Enable CDI feature for ContainerD",
            "text": "To use the DRA API, CDI must be enabled in the container runtime.\nContainerD supports CDI by default starting from version 2.0.0.\nFor earlier versions, you need to explicitly enable CDI in the config.toml configuration file, typically located at /etc/containerd/config.toml.\nTo enable CDI support, add the following configuration:\n[plugins.\"io.containerd.grpc.v1.cri\"]\nenable_cdi = true\ncdi_spec_dirs = [\"/etc/cdi\", \"/var/run/cdi\"]\nAfter modifying the configuration, restart ContainerD to apply the changes:\nsudo systemctl restart containerd\nFor more details, refer to the official ContainerD documentation on\nenabling CDI support\n.",
            "children": []
          },
          {
            "heading": "Kubeadm Configuration for DRA Enablement in Kubernetes v1.33",
            "text": "DRA is a Beta feature in v1.33, usually a beta feature is enabled by default, but DRA introduces so many API changes so it is not enabled by default in v1.33.\nTo enable DRA in Kubernetes v1.33, you need to explicitly enable few feature gates for api-server, controller-manager, scheduler and kubelet.\nThese configuration won't be necessary in v1.34, as DRA will be promoted to GA in v1.34 and enabled by default.\nFollowing instructions and example\nkubeadm\nconfiguration illustrate how to enable DRA related feature gates for each component in Kubernetes v1.33.\nPlease use valid settings for your cluster in all parts of ClusterConfiguration and InitConfiguration, except for the apiServer, controllerManager, and scheduler sections in ClusterConfiguration, and the kubeletExtraArgs setting in InitConfiguration.\ncat\n<<\nEOF\n> config.yaml\napiVersion: kubeadm.k8s.io/v1beta4\nkind: ClusterConfiguration\nkubernetesVersion: \"v1.33.2\"\nnetworking:\npodSubnet: \"10.244.0.0/16\"\napiServer:\nextraArgs:\n- name: feature-gates\nvalue: \"DynamicResourceAllocation=true,DRAResourceClaimDeviceStatus=true\"\n- name: runtime-config\nvalue: \"resource.k8s.io/v1beta1=true,resource.k8s.io/v1beta2=true\"\ncontrollerManager:\nextraArgs:\n- name: feature-gates\nvalue: \"DynamicResourceAllocation=true\"\nscheduler:\nextraArgs:\n- name: feature-gates\nvalue: \"DynamicResourceAllocation=true\"\n---\napiVersion: kubeadm.k8s.io/v1beta4\nkind: InitConfiguration\nnodeRegistration:\ncriSocket: \"/var/run/containerd/containerd.sock\"\nkubeletExtraArgs:\n- name: feature-gates\nvalue: \"DynamicResourceAllocation=true\"\nskipPhases:\n- addon/kube-proxy\nsudo kubeadm init --config dra.yaml\nOnce your cluster is successfully initialized you can simply verify DRA enablement with following command\nkubectl get deviceclasses\nIf the result is similar to the following, then DRA is enabled successfully.\nNo resources found\nIf you see an error like\nerror: the server doesn't have a resource type \"deviceclasses\"\n, then DRA is not enabled successfully, you may need to check the kubeadm configuration.\nYou can also enable the DRA feature manually by first creating the cluster using the default kubeadm settings, and then editing the configuration files of the kube-apiserver, kube-controller-manager, kube-scheduler, and kubelet directly to enable the required feature gates and API groups.\nSee\nenabling dynamic resource allocation\nfor more details.",
            "children": []
          },
          {
            "heading": "Installing Furiosa DRA Driver",
            "text": "Experimental version of Furiosa DRA Driver and Helm Chart for 2025.2.0 release are available now.\nDockerhub:\nhttps://hub.docker.com/r/furiosaai/furiosa-dra-driver/tags\nHelmChart:\nhttps://github.com/furiosa-ai/helm-charts/tree/main/charts/furiosa-dra-driver\nFollowing instructions illustrate how to install Furiosa DRA driver in Kubernetes v1.33 with Helm chart.\nWe recommend to install Furiosa DRA Driver along with Node Feature Discovery and furiosa-feature-discovery to ensure the DRA driver can discover Furiosa NPU devices correctly.\nFollowing instructions illustrate how to add the Helm repository and install Furiosa DRA driver with Node Feature Discovery and furiosa-feature-discovery.\nhelm repo add furiosa https://furiosa-ai.github.io/helm-charts\nhelm repo update\nhelm install -n kube-system furiosa-feature-discovery furiosa/furiosa-feature-discovery --version=2025.2.0\nhelm install -n kube-system furiosa-dra-driver furiosa/furiosa-dra-driver --version=2025.2.0\nIf you successfully installed Furiosa DRA driver, you can verify the installation with following commands.\nkubectl get deviceclasses\nNAME             AGE\nnpu.furiosa.ai   92m\n\nkubectl get resourceslice\nNAME                                 NODE            DRIVER           POOL            AGE\nrngd-tsvr-010-npu.furiosa.ai-bdb5l   rngd-tsvr-010   npu.furiosa.ai   rngd-tsvr-010   92m\nIf you want to deploy only Furiosa DRA driver without Node Feature Discovery and Furiosa Feature Discovery deployment, you can skip installing them.\nHowever, you need to remove the affinity section from the\nvalues.yaml\nfile of Furiosa DRA driver Helm chart.\naffinity\n:\nnodeAffinity\n:\nrequiredDuringSchedulingIgnoredDuringExecution\n:\nnodeSelectorTerms\n:\n          -\nmatchExpressions\n:\n              -\nkey\n:\nfeature.node.kubernetes.io/pci-1200_1ed2.present\noperator\n:\nIn\nvalues\n:\n                  -\n\"\ntrue\n\"",
            "children": [
              {
                "heading": "Requesting Furiosa NPU via DRA API",
                "text": "DRA API is inspired by the Persistent Volume API. So even though you are new to DRA API, you can easily understand its API pattern and usage.\nWe are going to use following new Kubernetes API to request Furiosa NPU devices.\nDeviceClass: Defines a class of devices, such as \"npu.furiosa.ai\".\nResourceSlice: Defines a collection of devices and its attributes and capacity.\nResourceClaim: Defines a claim for a specific resource, such as a Furiosa NPU device.\nResourceClaimTemplate: Defines a template for creating ResourceClaims, it is optional but recommended for some use cases such as requesting multiple devices with the same configuration.\nVisit\nDynamic Resource Allocation - API\nfor more details about DRA API.",
                "children": [
                  {
                    "heading": "Requesting Furiosa NPU using ResourceClaimTemplate",
                    "text": "The following example shows how to request a Furiosa NPU using a ResourceClaimTemplate.\nThis approach is convenient when requesting an NPU without specific requirements.\nIn this example, we define a basic ResourceClaimTemplate that can be reused by multiple Pods.\nEach time a Pod references the ResourceClaimTemplate to request a device, a new ResourceClaim is automatically created and bound to that Pod.\nWhen the Pod is deleted, the associated ResourceClaim is automatically cleaned up as well.\nRun the following command to create a ResourceClaimTemplate and two Pods that request a Furiosa NPU device.\nkubectl apply -f -\n<<\nEOF\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaimTemplate\nmetadata:\nname: single-npu\nspec:\nspec:\ndevices:\nrequests:\n- name: npu\ndeviceClassName: npu.furiosa.ai\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod1\nlabels:\napp: pod1\nspec:\ncontainers:\n- name: furiosa\nimage: furiosaai/furiosa-smi:2025.2.0\ncommand:\n- sleep\n- \"86400\"\nresources:\nclaims:\n- name: npu\nresourceClaims:\n- name: npu\nresourceClaimTemplateName: single-npu\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod2\nlabels:\napp: pod2\nspec:\ncontainers:\n- name: furiosa\nimage: furiosaai/furiosa-smi:2025.2.0\ncommand:\n- sleep\n- \"86400\"\nresources:\nclaims:\n- name: npu\nresourceClaims:\n- name: npu\nresourceClaimTemplateName: single-npu\nEOF\nIf you apply the above YAML, it will create a ResourceClaimTemplate named\nsingle-npu\n, and two Pods named\npod1\nand\npod2\nthat request a Furiosa NPU device using this template.\nYou can also see the automatically created ResourceClaims for each Pod.\nkubectl get resourceclaimtemplate\nNAME         AGE\nsingle-npu   12s\n\nkubectl get resourceclaim\nNAME             STATE                AGE\npod1-npu-hdbmx   allocated,reserved   17s\npod2-npu-jlfw9   allocated,reserved   17s\n\nkubectl get pod\nNAME   READY   STATUS    RESTARTS   AGE\npod1   1/1     Running   0          26s\npod2   1/1     Running   0          26s\nYou can also check the details of the resource allocation.\nIn this result,\nstatus.allocation.devices[0].results[0].device\nindicates that the RNGD(\nnpu3\n) of\nrngd-tsvr-010\nnode is allocated for the ResourceClaim\npod1-npu-hdbmx\nassociated with\npod1\n.\nkubectl get resourceclaim pod1-npu-hdbmx -o yaml\napiVersion: resource.k8s.io/v1beta2\nkind: ResourceClaim\nmetadata:\n  annotations:\n    resource.kubernetes.io/pod-claim-name: npu\n  creationTimestamp:\n\"\n2025-07-03T00:13:48Z\n\"\nfinalizers:\n  - resource.kubernetes.io/delete-protection\n  generateName: pod1-npu-\n  name: pod1-npu-hdbmx\n  namespace: default\n  ownerReferences:\n  - apiVersion: v1\n    blockOwnerDeletion:\ntrue\ncontroller:\ntrue\nkind: Pod\n    name: pod1\n    uid: 7cae21e9-1d91-47ef-81fe-655d3b8ca9b9\n  resourceVersion:\n\"\n22604\n\"\nuid: dbe93324-0afc-4cc2-83a1-ff9387ac38f8\nspec:\n  devices:\n    requests:\n    - exactly:\n        allocationMode: ExactCount\n        count: 1\n        deviceClassName: npu.furiosa.ai\n      name: npu\nstatus:\n  allocation:\n    devices:\n      results:\n      - adminAccess: null\n#\nthis field indicates which device is allocated for this ResourceClaim\ndevice: npu3\n        driver: npu.furiosa.ai\n        pool: rngd-tsvr-010\n        request: npu\n    nodeSelector:\n      nodeSelectorTerms:\n      - matchFields:\n        - key: metadata.name\n          operator: In\n          values:\n          - rngd-tsvr-010\n  reservedFor:\n  - name: pod1\n    resource: pods\n    uid: 7cae21e9-1d91-47ef-81fe-655d3b8ca9b9",
                    "children": []
                  },
                  {
                    "heading": "Requesting Furiosa NPU using ResourceClaim with specific requirements",
                    "text": "In some cases, you may want to request a Furiosa NPU with specific requirements, such as:\na specific device identified by UUID or Name\ndevices located in a specific NUMA node\ndevices using a specific driver version or firmware version\nIn this example, we request specific Furiosa NPU devices by UUID.\nInstead of using a ResourceClaimTemplate, we define each ResourceClaim explicitly, linking it to a unique device.\nFor demonstration, we create two ResourceClaims, each associated with a specific device:\nnpu0: UUID 40512C86-0705-4506-8E45-494644454A43\nnpu1: UUID 41512C86-0703-4208-8C43-49444C4C4E43\nKubernetes DRA supports using CEL expressions in the selectors field of a ResourceClaim to match devices based on their attributes.\nFor example, to select a specific device by UUID, you can use the following expression:\nexpression\n:\n\"\ndevice.attributes['npu.furiosa.ai'].uuid == '09512C86-070D-4204-8342-4C424747474B'\n\"\nRun the following command to create two ResourceClaims and two Pods, where each Pod is configured to requests a specific Furiosa NPU device through its associated ResourceClaim.\nkubectl apply -f -\n<<\nEOF\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaim\nmetadata:\nname: npu0-claim\nspec:\ndevices:\nrequests:\n- name: rngd-npu\ndeviceClassName: npu.furiosa.ai\nallocationMode: ExactCount\nselectors:\n- cel:\nexpression: \"device.attributes['npu.furiosa.ai'].uuid == '40512C86-0705-4506-8E45-494644454A43'\"\n---\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaim\nmetadata:\nname: npu1-claim\nspec:\ndevices:\nrequests:\n- name: rngd-npu\ndeviceClassName: npu.furiosa.ai\nallocationMode: ExactCount\nselectors:\n- cel:\nexpression: \"device.attributes['npu.furiosa.ai'].uuid == '41512C86-0703-4208-8C43-49444C4C4E43'\"\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod1\nlabels:\napp: pod1\nspec:\ncontainers:\n- name: furiosa\nimage: furiosaai/furiosa-smi:2025.2.0\ncommand:\n- sleep\n- \"86400\"\nresources:\nclaims:\n- name: npu\nresourceClaims:\n- name: npu\nresourceClaimName: npu0-claim\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod2\nlabels:\napp: pod2\nspec:\ncontainers:\n- name: furiosa\nimage: furiosaai/furiosa-smi:2025.2.0\ncommand:\n- sleep\n- \"86400\"\nresources:\nclaims:\n- name: npu\nresourceClaims:\n- name: npu\nresourceClaimName: npu1-claim\nEOF\nFollowing command will show the created ResourceClaims and Pods.\nYou can also check the\nstatus.allocation.devices[0].results[0].device\nfield of each ResourceClaim to see which device is allocated to each Pod.\nkubectl get pod\nNAME   READY   STATUS    RESTARTS   AGE\npod1   1/1     Running   0          2m12s\npod2   1/1     Running   0          2m12s\n\nkubectl get resourceclaim\nNAME         STATE                AGE\nnpu0-claim   allocated,reserved   11s\nnpu1-claim   allocated,reserved   11s\nNote that when a ResourceClaim is created explicitly, it is not automatically deleted even if the Pod referencing it is removed.\nIn the example below, each Pod that references a ResourceClaim is deleted. The Claims are then released, but remain in the Pending state:\nkubectl delete pod pod1 pod2\npod\n\"\npod1\n\"\ndeleted\npod\n\"\npod2\n\"\ndeleted\n\nkubectl get resourceclaim\nNAME         STATE     AGE\nnpu0-claim   pending   5m26s\nnpu1-claim   pending   5m26s\nThis approach makes it possible to pre-create ResourceClaim objects for specific devices, such as those identified by UUID or name, and have Pods consume them only when needed.\nIt follows a usage pattern similar to how PersistentVolumeClaims (PVCs) are used in Kubernetes: you can declare the resource ahead of time and bind it to a Pod later, giving you more control and flexibility over device allocation.",
                    "children": []
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "metadata": {
    "description": null,
    "license": null,
    "stars": 1,
    "forks": 0,
    "topics": [],
    "last_updated": "2025-11-14T00:39:13.502410Z"
  }
}